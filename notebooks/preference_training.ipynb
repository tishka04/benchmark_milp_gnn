{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-mgyGeo0lpx"
      },
      "source": [
        "# Preference-Based EBM Training for Power System Decisions\n",
        "\n",
        "This notebook trains a conditional Energy-Based Model (EBM) using preference learning with an LP economic oracle.\n",
        "\n",
        "**Methodology (from preference_learning.tex):**\n",
        "1. **HTE Embeddings**: Encode scenarios into context vectors `h = HTE(x)`\n",
        "2. **EBM with GRU**: Learn energy function `E_Œ∏(u | h)` over decisions with temporal understanding\n",
        "3. **Langevin Sampling**: Generate K candidate decisions `{u^(k)} ~ S_Œ∏(h)`\n",
        "4. **LP Worker**: Evaluate candidates with physics-aware economic oracle\n",
        "5. **Preference Learning**: Margin ranking loss to shape energy landscape\n",
        "\n",
        "**Target Hardware:** Colab A100 80GB VRAM\n",
        "\n",
        "**Contents:**\n",
        "1. Setup & Installation\n",
        "2. Configuration\n",
        "3. Data Loading\n",
        "4. Model Initialization\n",
        "5. Training Loop\n",
        "6. Evaluation & Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgqo7ayj0lpz"
      },
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KD0NiiI0lp0",
        "outputId": "d1cb20ae-c2cf-4435-fcfd-384a0633be54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "‚úì Working directory: /content/drive/MyDrive/benchmark\n",
            "‚úì PyTorch version: 2.9.0+cu126\n",
            "‚úì CUDA available: True\n",
            "‚úì GPU: NVIDIA A100-SXM4-80GB\n",
            "‚úì GPU Memory: 85.2 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 1. SETUP & INSTALLATION\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Check if running on Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    # Set repository path\n",
        "    REPO_PATH = '/content/drive/MyDrive/benchmark'\n",
        "\n",
        "    if not os.path.exists(REPO_PATH):\n",
        "        print(\"Please upload the benchmark folder to Google Drive at: MyDrive/benchmark/\")\n",
        "        print(\"Or modify REPO_PATH to point to your repository location\")\n",
        "    else:\n",
        "        sys.path.insert(0, REPO_PATH)\n",
        "        os.chdir(REPO_PATH)\n",
        "        print(f\"‚úì Working directory: {os.getcwd()}\")\n",
        "\n",
        "    # Install dependencies using %pip (Jupyter magic)\n",
        "    get_ipython().system('pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121')\n",
        "    get_ipython().system('pip install -q torch-geometric pyomo highspy tqdm matplotlib seaborn pandas numpy')\n",
        "\n",
        "else:\n",
        "    # Local setup\n",
        "    REPO_PATH = r'C:\\Users\\Dell\\projects\\multilayer_milp_gnn\\benchmark'\n",
        "    sys.path.insert(0, REPO_PATH)\n",
        "    os.chdir(REPO_PATH)\n",
        "    print(f\"‚úì Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Verify GPU\n",
        "import torch\n",
        "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1I-zs2_0lp2"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2K1JAc00lp2",
        "outputId": "16bbc22c-2ded-4c79-a331-759837200412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CONFIGURATION\n",
            "============================================================\n",
            "Architecture: gru\n",
            "Device: cuda\n",
            "Epochs: 50\n",
            "Batch size: 2\n",
            "Candidates per scenario: 3\n",
            "Langevin steps: 30\n",
            "Use LP Worker: True\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 2. CONFIGURATION (TWO-PHASE TRAINING)\n",
        "# ============================================================================\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Any\n",
        "from pathlib import Path\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Training configuration for two-phase preference-based EBM.\"\"\"\n",
        "\n",
        "    # Paths\n",
        "    scenarios_dir: str = \"outputs/scenarios_v1_filtered\"\n",
        "    milp_reports_dir: str = \"outputs/scenarios_v1_filtered/reports\"\n",
        "    embeddings_path: Optional[str] = \"outputs/encoders/hierachical_temporal/embeddings_multiscale_normalized.pt\"\n",
        "    prebuilt_lp_dir: str = \"outputs/lp_models_v1\"  # Pre-built LP models\n",
        "    output_dir: str = \"outputs/preference_training\"\n",
        "\n",
        "    # Data\n",
        "    train_split: float = 0.8\n",
        "\n",
        "    # Model architecture\n",
        "    architecture: str = \"gru\"  # \"mlp\", \"gru\", \"transformer\"\n",
        "    h_dim: int = 128\n",
        "    hidden_dim: int = 128\n",
        "    gru_layers: int = 2\n",
        "    num_heads: int = 4\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    # Sampling (defaults for model init - phases override these)\n",
        "    langevin_steps: int = 20\n",
        "    langevin_step_size: float = 0.01\n",
        "    langevin_noise: float = 0.01\n",
        "    num_candidates: int = 3\n",
        "\n",
        "    # ============= PHASE 1: PRETRAIN (fast, no LP) =============\n",
        "    pretrain_epochs: int = 15\n",
        "    pretrain_lr: float = 1e-3\n",
        "    pretrain_langevin_steps: int = 10\n",
        "    pretrain_num_candidates: int = 3\n",
        "    \n",
        "    # ============= PHASE 2: FINETUNE (with LP) =============\n",
        "    finetune_epochs: int = 10\n",
        "    finetune_lr: float = 1e-4\n",
        "    finetune_langevin_steps: int = 20\n",
        "    finetune_lp_ratio: float = 0.2           # LP on 20% of batches\n",
        "    finetune_uncertainty_top: float = 0.3     # Only top 30% uncertain scenarios\n",
        "    finetune_num_candidates_min: int = 2\n",
        "    finetune_num_candidates_max: int = 8      # Adaptive K\n",
        "    finetune_cost_proxy_top_k: int = 2        # Filter to top-2 before LP\n",
        "\n",
        "    # Loss\n",
        "    margin: float = 1.0\n",
        "    alpha: float = 1.0\n",
        "    w_max: float = 5.0\n",
        "    energy_reg: float = 0.01\n",
        "\n",
        "    # Optimization\n",
        "    learning_rate: float = 1e-4  # Default for model init\n",
        "    weight_decay: float = 1e-5\n",
        "    gradient_clip: float = 1.0\n",
        "    batch_size: int = 4\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 25  # Total epochs (pretrain + finetune)\n",
        "    eval_every: int = 5\n",
        "    save_every: int = 5\n",
        "    log_every: int = 1\n",
        "\n",
        "    # LP Oracle\n",
        "    use_lp_worker: bool = True\n",
        "    lp_solver: str = \"appsi_highs\"\n",
        "    lp_time_limit: float = 0.5\n",
        "    lp_cache_size: int = 100\n",
        "\n",
        "    # Device\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Scenario dimensions (set from data)\n",
        "    n_zones: int = 100\n",
        "    n_timesteps: int = 24\n",
        "    n_features: int = 8\n",
        "\n",
        "# Create config\n",
        "config = Config()\n",
        "\n",
        "# Create output directory\n",
        "Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TWO-PHASE TRAINING CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üìç Device: {config.device}\")\n",
        "print(f\"üì¶ Batch size: {config.batch_size}\")\n",
        "print()\n",
        "print(\"üöÄ PHASE 1: PRETRAIN (fast, no LP)\")\n",
        "print(f\"   Epochs: {config.pretrain_epochs}\")\n",
        "print(f\"   Learning rate: {config.pretrain_lr}\")\n",
        "print(f\"   Langevin steps: {config.pretrain_langevin_steps}\")\n",
        "print(f\"   Candidates (K): {config.pretrain_num_candidates}\")\n",
        "print()\n",
        "print(\"üéØ PHASE 2: FINETUNE (with LP Oracle)\")\n",
        "print(f\"   Epochs: {config.finetune_epochs}\")\n",
        "print(f\"   Learning rate: {config.finetune_lr}\")\n",
        "print(f\"   Langevin steps: {config.finetune_langevin_steps}\")\n",
        "print(f\"   LP batch ratio: {config.finetune_lp_ratio:.0%}\")\n",
        "print(f\"   Uncertainty top: {config.finetune_uncertainty_top:.0%}\")\n",
        "print(f\"   Candidates (K): {config.finetune_num_candidates_min} ‚Üí {config.finetune_num_candidates_max}\")\n",
        "print(f\"   Cost proxy filter: top-{config.finetune_cost_proxy_top_k}\")\n",
        "print()\n",
        "print(f\"üìÅ Pre-built LP models: {config.prebuilt_lp_dir}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMwQ9-Wz0lp3"
      },
      "source": [
        "## 3. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6wQPpHL0lp3",
        "outputId": "1fa23ee1-db1b-43a1-9a33-8c825393c974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scenarios directory: outputs/scenarios_v1_filtered (exists: True)\n",
            "Reports directory: outputs/scenarios_v1_filtered/reports (exists: True)\n",
            "Found 2960 scenario files\n",
            "Found 2960 MILP reports\n",
            "\n",
            "üìä Zone count analysis (from 100 scenarios):\n",
            "   Min zones: 8\n",
            "   Max zones: 119\n",
            "   Mean zones: 58.4\n",
            "   Median zones: 56\n",
            "\n",
            "‚úì Config updated:\n",
            "   max_zones: 129 (for padding)\n",
            "   n_zones: DYNAMIC (varies per scenario)\n",
            "   n_timesteps: 24\n",
            "   n_features: 8\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 3. DATA LOADING\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Import preference learning modules\n",
        "from src.preference.data_models import ScenarioData, DecisionVector\n",
        "\n",
        "# Check if data exists\n",
        "scenarios_path = Path(config.scenarios_dir)\n",
        "reports_path = Path(config.milp_reports_dir)\n",
        "\n",
        "print(f\"Scenarios directory: {scenarios_path} (exists: {scenarios_path.exists()})\")\n",
        "print(f\"Reports directory: {reports_path} (exists: {reports_path.exists()})\")\n",
        "\n",
        "# Count available data\n",
        "scenario_files = []\n",
        "if scenarios_path.exists():\n",
        "    scenario_files = sorted(list(scenarios_path.glob(\"scenario_*.json\")))\n",
        "    print(f\"Found {len(scenario_files)} scenario files\")\n",
        "\n",
        "report_files = []\n",
        "if reports_path.exists():\n",
        "    report_files = list(reports_path.glob(\"scenario_*.json\"))\n",
        "    print(f\"Found {len(report_files)} MILP reports\")\n",
        "\n",
        "# Analyze zone counts across scenarios (they are DYNAMIC)\n",
        "def get_n_zones(scenario_path):\n",
        "    \"\"\"Extract number of zones from scenario file.\"\"\"\n",
        "    with open(scenario_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    # zones_per_region contains list of zone counts per region\n",
        "    zones_per_region = data.get(\"graph\", {}).get(\"zones_per_region\", [])\n",
        "    return sum(zones_per_region)\n",
        "\n",
        "# Sample zone counts from first N scenarios\n",
        "if scenario_files:\n",
        "    sample_size = min(100, len(scenario_files))\n",
        "    zone_counts = [get_n_zones(f) for f in scenario_files[:sample_size]]\n",
        "\n",
        "    print(f\"\\nüìä Zone count analysis (from {sample_size} scenarios):\")\n",
        "    print(f\"   Min zones: {min(zone_counts)}\")\n",
        "    print(f\"   Max zones: {max(zone_counts)}\")\n",
        "    print(f\"   Mean zones: {np.mean(zone_counts):.1f}\")\n",
        "    print(f\"   Median zones: {np.median(zone_counts):.0f}\")\n",
        "\n",
        "    # Set max_zones for padding (use max + buffer)\n",
        "    config.max_zones = max(zone_counts) + 10\n",
        "    config.n_zones = None  # Dynamic - will be determined per batch\n",
        "\n",
        "    # Load first scenario for timesteps\n",
        "    with open(scenario_files[0], 'r') as f:\n",
        "        sample_scenario = json.load(f)\n",
        "    config.n_timesteps = sample_scenario.get(\"horizon_hours\", 24)\n",
        "\n",
        "    print(f\"\\n‚úì Config updated:\")\n",
        "    print(f\"   max_zones: {config.max_zones} (for padding)\")\n",
        "    print(f\"   n_zones: DYNAMIC (varies per scenario)\")\n",
        "    print(f\"   n_timesteps: {config.n_timesteps}\")\n",
        "    print(f\"   n_features: {config.n_features}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No scenario files found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8Sg3nxJ0lp4",
        "outputId": "30b5771e-7272-4748-ceed-d8d080b1044e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Found zone embeddings at outputs/encoders/hierarchical_temporal/embeddings_zone.pt\n",
            "üìÇ Loading zone embeddings from outputs/encoders/hierarchical_temporal/embeddings_zone.pt...\n",
            "‚úì Loaded 2960 scenario embeddings\n",
            "\n",
            "‚úì Zone embeddings loaded: 2960 scenarios\n",
            "   Sample shape: torch.Size([52, 24, 128]) (Z, T, D)\n",
            "   Embed dim: 128\n"
          ]
        }
      ],
      "source": [
        "# Create dataset with zone-level embeddings support\n",
        "from src.preference.embedding_processor import (\n",
        "    ZonalEmbeddingProcessor,\n",
        "    TemporalZonalDataset,\n",
        "    temporal_collate_fn,\n",
        "    load_zone_embeddings,\n",
        ")\n",
        "\n",
        "# Check for pre-processed zone embeddings\n",
        "ZONE_EMBEDDING_PATH = \"outputs/encoders/hierarchical_temporal/embeddings_zone.pt\"\n",
        "MULTISCALE_EMBEDDING_PATH = \"outputs/encoders/hierarchical_temporal/embeddings_multiscale_normalized.pt\"\n",
        "DATASET_INDEX_PATH = \"outputs/graphs/hetero_temporal_v1/dataset_index.json\"\n",
        "\n",
        "zone_embeddings = None\n",
        "\n",
        "# Try to load zone embeddings\n",
        "if Path(ZONE_EMBEDDING_PATH).exists():\n",
        "    print(f\"‚úì Found zone embeddings at {ZONE_EMBEDDING_PATH}\")\n",
        "    zone_embeddings = load_zone_embeddings(ZONE_EMBEDDING_PATH)\n",
        "    config.embeddings_path = ZONE_EMBEDDING_PATH\n",
        "\n",
        "elif Path(MULTISCALE_EMBEDDING_PATH).exists():\n",
        "    print(f\"üîß Processing multiscale embeddings to zone level...\")\n",
        "    processor = ZonalEmbeddingProcessor()\n",
        "    zone_embeddings = processor.load_and_process(\n",
        "        embedding_path=MULTISCALE_EMBEDDING_PATH,\n",
        "        dataset_index_path=DATASET_INDEX_PATH if Path(DATASET_INDEX_PATH).exists() else None,\n",
        "        output_path=ZONE_EMBEDDING_PATH,\n",
        "        level=\"zones\",\n",
        "    )\n",
        "    config.embeddings_path = ZONE_EMBEDDING_PATH\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No embeddings found - will use dummy embeddings\")\n",
        "    print(f\"   Expected: {ZONE_EMBEDDING_PATH}\")\n",
        "    print(f\"   Or: {MULTISCALE_EMBEDDING_PATH}\")\n",
        "\n",
        "# Show embedding info\n",
        "if zone_embeddings:\n",
        "    sample_key = next(iter(zone_embeddings.keys()))\n",
        "    sample_emb = zone_embeddings[sample_key]\n",
        "    print(f\"\\n‚úì Zone embeddings loaded: {len(zone_embeddings)} scenarios\")\n",
        "    print(f\"   Sample shape: {sample_emb.shape} (Z, T, D)\")\n",
        "    print(f\"   Embed dim: {sample_emb.shape[-1]}\")\n",
        "\n",
        "    # Update config\n",
        "    config.h_dim = sample_emb.shape[-1]\n",
        "    if sample_emb.dim() >= 2:\n",
        "        config.n_timesteps = sample_emb.shape[1] if sample_emb.dim() == 3 else 24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5BZR0IF0lp5",
        "outputId": "7aa194fc-bb08-4913-84c4-0a4c21be499e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating TemporalZonalDataset with zone embeddings...\n",
            "‚úì TemporalZonalDataset: 2960 scenarios\n",
            "\n",
            "‚úì Dataset created: 2960 samples\n",
            "   Sample u_zt shape: torch.Size([52, 24, 8])\n",
            "   Sample h_zt shape: torch.Size([52, 24, 128])\n",
            "   n_zones: 52 (DYNAMIC)\n",
            "\n",
            "üìä Zone count distribution in dataset:\n",
            "   First 50: min=8, max=118, mean=61.4\n"
          ]
        }
      ],
      "source": [
        "# Create TemporalZonalDataset if zone embeddings are available\n",
        "# Otherwise create a SimpleDataset that loads from scenarios directly\n",
        "\n",
        "if zone_embeddings:\n",
        "    print(\"üîß Creating TemporalZonalDataset with zone embeddings...\")\n",
        "\n",
        "    temporal_dataset = TemporalZonalDataset(\n",
        "        scenarios_dir=config.scenarios_dir,\n",
        "        zone_embeddings=zone_embeddings,\n",
        "        milp_reports_dir=config.milp_reports_dir,\n",
        "        n_features=config.n_features,\n",
        "        n_timesteps=config.n_timesteps,\n",
        "    )\n",
        "\n",
        "    if len(temporal_dataset) > 0:\n",
        "        sample0 = temporal_dataset[0]\n",
        "        print(f\"\\n‚úì Dataset created: {len(temporal_dataset)} samples\")\n",
        "        print(f\"   Sample u_zt shape: {sample0['u_zt'].shape}\")\n",
        "        print(f\"   Sample h_zt shape: {sample0['h_zt'].shape}\")\n",
        "        print(f\"   n_zones: {sample0['n_zones']} (DYNAMIC)\")\n",
        "\n",
        "        config.h_dim = sample0['embed_dim']\n",
        "else:\n",
        "    print(\"üîß Creating SimplePreferenceDataset from scenarios...\")\n",
        "\n",
        "    # Simple dataset that loads directly from scenario files\n",
        "    class SimplePreferenceDataset(Dataset):\n",
        "        \"\"\"Dataset loading scenarios and MILP reports with dynamic zone counts.\"\"\"\n",
        "\n",
        "        def __init__(self, scenario_files, reports_dir, n_features=8, n_timesteps=24, h_dim=128):\n",
        "            self.scenario_files = scenario_files\n",
        "            self.reports_dir = Path(reports_dir)\n",
        "            self.n_features = n_features\n",
        "            self.n_timesteps = n_timesteps\n",
        "            self.h_dim = h_dim\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.scenario_files)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            scenario_path = self.scenario_files[idx]\n",
        "            scenario_id = scenario_path.stem\n",
        "\n",
        "            # Load scenario\n",
        "            with open(scenario_path, 'r') as f:\n",
        "                scenario = json.load(f)\n",
        "\n",
        "            # Get zone count\n",
        "            zones_per_region = scenario.get(\"graph\", {}).get(\"zones_per_region\", [])\n",
        "            n_zones = sum(zones_per_region)\n",
        "\n",
        "            # Load MILP report\n",
        "            report_path = self.reports_dir / f\"{scenario_id}.json\"\n",
        "            milp_objective = float(\"inf\")\n",
        "            milp_decisions = torch.zeros(n_zones, self.n_timesteps, self.n_features)\n",
        "\n",
        "            if report_path.exists():\n",
        "                with open(report_path, 'r') as f:\n",
        "                    report = json.load(f)\n",
        "                milp_objective = report.get(\"mip\", {}).get(\"objective\", float(\"inf\"))\n",
        "\n",
        "                # Extract binary decisions from dispatch\n",
        "                dispatch = report.get(\"dispatch\", {})\n",
        "                if dispatch:\n",
        "                    milp_decisions = self._extract_decisions(dispatch, n_zones)\n",
        "\n",
        "            # Dummy embedding (will be replaced with real HTE embeddings)\n",
        "            h = torch.randn(n_zones, self.n_timesteps, self.h_dim)\n",
        "\n",
        "            return {\n",
        "                'scenario_id': scenario_id,\n",
        "                'u_zt': milp_decisions.float(),\n",
        "                'h_zt': h.float(),\n",
        "                'n_zones': n_zones,\n",
        "                'n_timesteps': self.n_timesteps,\n",
        "                'n_features': self.n_features,\n",
        "                'embed_dim': self.h_dim,\n",
        "                'milp_objective': milp_objective,\n",
        "            }\n",
        "\n",
        "        def _extract_decisions(self, dispatch, n_zones):\n",
        "            \"\"\"Extract binary decisions from MILP dispatch.\"\"\"\n",
        "            T = self.n_timesteps\n",
        "            decisions = torch.zeros(n_zones, T, self.n_features)\n",
        "\n",
        "            # Feature mapping\n",
        "            feature_keys = [\n",
        "                'battery_charge', 'battery_discharge',\n",
        "                'pumped_charge', 'pumped_discharge',\n",
        "                'dr_active', 'nuclear', 'thermal', 'import_mode'\n",
        "            ]\n",
        "\n",
        "            for f_idx, key in enumerate(feature_keys):\n",
        "                if key in dispatch:\n",
        "                    data = dispatch[key]\n",
        "                    if isinstance(data, list):\n",
        "                        arr = np.array(data)\n",
        "                        if arr.ndim == 2:\n",
        "                            Z_data, T_data = arr.shape\n",
        "                            Z_use = min(Z_data, n_zones)\n",
        "                            T_use = min(T_data, T)\n",
        "                            decisions[:Z_use, :T_use, f_idx] = torch.from_numpy(arr[:Z_use, :T_use])\n",
        "\n",
        "            return decisions\n",
        "\n",
        "    # Create dataset using all available scenarios\n",
        "    temporal_dataset = SimplePreferenceDataset(\n",
        "        scenario_files=scenario_files,\n",
        "        reports_dir=config.milp_reports_dir,\n",
        "        n_features=config.n_features,\n",
        "        n_timesteps=config.n_timesteps,\n",
        "        h_dim=config.h_dim,\n",
        "    )\n",
        "\n",
        "    if len(temporal_dataset) > 0:\n",
        "        sample0 = temporal_dataset[0]\n",
        "        print(f\"\\n‚úì Dataset created: {len(temporal_dataset)} samples\")\n",
        "        print(f\"   Sample u_zt shape: {sample0['u_zt'].shape}\")\n",
        "        print(f\"   Sample h_zt shape: {sample0['h_zt'].shape}\")\n",
        "        print(f\"   n_zones: {sample0['n_zones']} (DYNAMIC)\")\n",
        "\n",
        "# Verify zone count distribution\n",
        "if len(temporal_dataset) > 0:\n",
        "    print(\"\\nüìä Zone count distribution in dataset:\")\n",
        "    zone_counts = []\n",
        "    for i in range(min(50, len(temporal_dataset))):\n",
        "        zone_counts.append(temporal_dataset[i]['n_zones'])\n",
        "    print(f\"   First 50: min={min(zone_counts)}, max={max(zone_counts)}, mean={np.mean(zone_counts):.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qnMZAKy0lp6",
        "outputId": "135c6c61-83e2-454b-9d6e-6c0e72440aa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìê Model dimensions:\n",
            "   max_zones: 129\n",
            "   n_timesteps: 24\n",
            "   n_features: 8\n",
            "   max_decision_dim: 24768\n",
            "   h_dim: 128\n",
            "\n",
            "‚úì EBM Model: ConditionalEBMWithGRU\n",
            "  Parameters: 669,825 (669,825 trainable)\n",
            "‚úì Langevin Sampler: 30 steps\n",
            "‚úì Loss: CombinedPreferenceLoss (margin=1.0, Œ±=1.0)\n",
            "‚úì Optimizer: AdamW (lr=0.0001)\n",
            "‚úì Scheduler: CosineAnnealingLR\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 4. MODEL INITIALIZATION (with dynamic zone support)\n",
        "# ============================================================================\n",
        "\n",
        "from src.preference.ebm import ConditionalEBMWithGRU, ConditionalEBM, build_ebm\n",
        "from src.preference.sampler import LangevinSampler, SamplerConfig, binarize_decisions\n",
        "from src.preference.losses import CombinedPreferenceLoss\n",
        "\n",
        "# Get max_zones from dataset analysis for model sizing\n",
        "# The model will use flattened decisions, so we need a max size\n",
        "max_zones_in_data = config.max_zones if hasattr(config, 'max_zones') else 100\n",
        "\n",
        "# Build EBM model - use max_zones for model capacity\n",
        "ebm_config = {\n",
        "    \"architecture\": config.architecture,\n",
        "    \"hidden_dim\": config.hidden_dim,\n",
        "    \"gru_layers\": config.gru_layers,\n",
        "    \"num_heads\": config.num_heads,\n",
        "    \"dropout\": config.dropout,\n",
        "    \"bidirectional\": True,\n",
        "    \"use_zone_attention\": True,\n",
        "}\n",
        "\n",
        "# Decision dim based on max possible size (will be padded in batch)\n",
        "max_decision_dim = max_zones_in_data * config.n_timesteps * config.n_features\n",
        "\n",
        "print(f\"üìê Model dimensions:\")\n",
        "print(f\"   max_zones: {max_zones_in_data}\")\n",
        "print(f\"   n_timesteps: {config.n_timesteps}\")\n",
        "print(f\"   n_features: {config.n_features}\")\n",
        "print(f\"   max_decision_dim: {max_decision_dim}\")\n",
        "print(f\"   h_dim: {config.h_dim}\")\n",
        "\n",
        "ebm = build_ebm(\n",
        "    config=ebm_config,\n",
        "    h_dim=config.h_dim,\n",
        "    decision_dim=max_decision_dim,\n",
        "    n_zones=max_zones_in_data,\n",
        "    n_timesteps=config.n_timesteps,\n",
        "    n_features=config.n_features,\n",
        ").to(config.device)\n",
        "\n",
        "# Count parameters\n",
        "n_params = sum(p.numel() for p in ebm.parameters())\n",
        "n_trainable = sum(p.numel() for p in ebm.parameters() if p.requires_grad)\n",
        "print(f\"\\n‚úì EBM Model: {type(ebm).__name__}\")\n",
        "print(f\"  Parameters: {n_params:,} ({n_trainable:,} trainable)\")\n",
        "\n",
        "# Create Langevin sampler\n",
        "sampler_config = SamplerConfig(\n",
        "    num_steps=config.langevin_steps,\n",
        "    step_size=config.langevin_step_size,\n",
        "    noise_scale=config.langevin_noise,\n",
        "    use_normalized=True,\n",
        "    anneal_schedule=\"cosine\",\n",
        ")\n",
        "sampler = LangevinSampler(ebm, sampler_config)\n",
        "print(f\"‚úì Langevin Sampler: {config.langevin_steps} steps\")\n",
        "\n",
        "# Create loss function\n",
        "loss_fn = CombinedPreferenceLoss(\n",
        "    margin=config.margin,\n",
        "    alpha=config.alpha,\n",
        "    w_max=config.w_max,\n",
        "    energy_reg_weight=config.energy_reg,\n",
        ")\n",
        "print(f\"‚úì Loss: CombinedPreferenceLoss (margin={config.margin}, Œ±={config.alpha})\")\n",
        "\n",
        "# Create optimizer and scheduler\n",
        "optimizer = torch.optim.AdamW(\n",
        "    ebm.parameters(),\n",
        "    lr=config.learning_rate,\n",
        "    weight_decay=config.weight_decay,\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=config.epochs,\n",
        "    eta_min=config.learning_rate * 0.01,\n",
        ")\n",
        "print(f\"‚úì Optimizer: AdamW (lr={config.learning_rate})\")\n",
        "print(f\"‚úì Scheduler: CosineAnnealingLR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flNtZ8IS0lp6",
        "outputId": "20117de6-7649-49ca-cbfd-63456e6507a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Initializing LP Oracle with full LP Worker...\n",
            "‚úì LP Oracle ready\n",
            "   Mode: Full LP\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 4b. LP ORACLE INITIALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "from src.preference.lp_oracle import (\n",
        "    LPOracle,\n",
        "    PreferenceLPOracle,\n",
        "    LPOracleConfig,\n",
        "    create_lp_oracle,\n",
        ")\n",
        "\n",
        "# Initialize LP Oracle\n",
        "if config.use_lp_worker:\n",
        "    print(\"üîß Initializing LP Oracle with full LP Worker...\")\n",
        "    lp_oracle = create_lp_oracle(\n",
        "    scenarios_dir=config.scenarios_dir,\n",
        "    solver_name=\"appsi_highs\",\n",
        "    use_full_lp=True,\n",
        "    use_cache=True,\n",
        "    max_cache_size=100,\n",
        "    prebuilt_dir=\"outputs/lp_models_v1\",  # ‚Üê Mod√®les pr√©-construits\n",
        "    time_limit_hard_fix=1,\n",
        "    time_limit_repair=0.5,\n",
        "    time_limit_full_soft=1,\n",
        "    verbose=False,\n",
        "    )\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è LP Worker disabled - using dummy costs\")\n",
        "    print(\"   Set config.use_lp_worker = True for real cost evaluation\")\n",
        "    lp_oracle = create_lp_oracle(\n",
        "        scenarios_dir=config.scenarios_dir,\n",
        "        use_full_lp=False,  # Will use dummy evaluation\n",
        "    )\n",
        "\n",
        "print(f\"‚úì LP Oracle ready\")\n",
        "print(f\"   Mode: {'Full LP' if config.use_lp_worker else 'Dummy costs'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWAca8084OO7",
        "outputId": "0324eb79-3f2c-430d-87fb-4e547eac0ce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Dataset split:\n",
            "   Train: 2368 scenarios\n",
            "   Eval: 592 scenarios\n",
            "‚úì Train loader: 1184 batches\n",
            "‚úì Eval loader: 296 batches\n",
            "\n",
            "üìã Test batch info:\n",
            "   milp_decisions shape: torch.Size([2, 73, 24, 8])\n",
            "   embeddings shape: torch.Size([2, 73, 24, 128])\n",
            "   zone_mask shape: torch.Size([2, 73])\n",
            "   n_zones: [70, 73]\n",
            "   max_zones in batch: 73\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 5. DATA LOADERS WITH DYNAMIC ZONE PADDING\n",
        "# ============================================================================\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def dynamic_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for variable-size scenarios with dynamic zone counts.\n",
        "    Pads all tensors to max zones in batch.\n",
        "    \"\"\"\n",
        "    # Find max zones in this batch\n",
        "    max_zones = max(item[\"n_zones\"] for item in batch)\n",
        "    n_timesteps = batch[0][\"n_timesteps\"]\n",
        "    n_features = batch[0][\"n_features\"]\n",
        "    embed_dim = batch[0][\"embed_dim\"]\n",
        "\n",
        "    u_batch = []\n",
        "    h_batch = []\n",
        "    zone_masks = []\n",
        "    n_zones_list = []\n",
        "    objectives = []\n",
        "    scenario_ids = []\n",
        "\n",
        "    for item in batch:\n",
        "        Z = item[\"n_zones\"]\n",
        "        pad_z = max_zones - Z\n",
        "\n",
        "        # Pad u_zt: [Z, T, F] -> [Z_max, T, F]\n",
        "        u = item[\"u_zt\"]\n",
        "        if pad_z > 0:\n",
        "            u = F.pad(u, (0, 0, 0, 0, 0, pad_z))\n",
        "        u_batch.append(u)\n",
        "\n",
        "        # Pad h_zt: [Z, T, D] -> [Z_max, T, D]\n",
        "        h = item[\"h_zt\"]\n",
        "        if pad_z > 0:\n",
        "            h = F.pad(h, (0, 0, 0, 0, 0, pad_z))\n",
        "        h_batch.append(h)\n",
        "\n",
        "        # Zone mask: [Z_max] - 1 for real zones, 0 for padding\n",
        "        mask = torch.cat([torch.ones(Z), torch.zeros(pad_z)])\n",
        "        zone_masks.append(mask)\n",
        "\n",
        "        n_zones_list.append(Z)\n",
        "        objectives.append(item[\"milp_objective\"])\n",
        "        scenario_ids.append(item[\"scenario_id\"])\n",
        "\n",
        "    return {\n",
        "        \"scenario_ids\": scenario_ids,\n",
        "        \"milp_decisions\": torch.stack(u_batch),      # [B, Z_max, T, F]\n",
        "        \"embeddings\": torch.stack(h_batch),          # [B, Z_max, T, D]\n",
        "        \"zone_mask\": torch.stack(zone_masks),        # [B, Z_max]\n",
        "        \"milp_objectives\": torch.tensor(objectives), # [B]\n",
        "        \"n_zones\": n_zones_list,                     # List[int] - original zone counts\n",
        "        \"n_timesteps\": n_timesteps,\n",
        "        \"n_features\": n_features,\n",
        "        \"embed_dim\": embed_dim,\n",
        "        \"max_zones\": max_zones,\n",
        "    }\n",
        "\n",
        "# Split dataset into train/eval\n",
        "n_total = len(temporal_dataset)\n",
        "n_train = int(n_total * config.train_split)\n",
        "n_eval = n_total - n_train\n",
        "\n",
        "train_indices = list(range(n_train))\n",
        "eval_indices = list(range(n_train, n_total))\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(temporal_dataset, train_indices)\n",
        "eval_dataset = torch.utils.data.Subset(temporal_dataset, eval_indices)\n",
        "\n",
        "print(f\"üìä Dataset split:\")\n",
        "print(f\"   Train: {len(train_dataset)} scenarios\")\n",
        "print(f\"   Eval: {len(eval_dataset)} scenarios\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=dynamic_collate_fn,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if config.device == \"cuda\" else False,\n",
        ")\n",
        "\n",
        "eval_loader = DataLoader(\n",
        "    eval_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=dynamic_collate_fn,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "print(f\"‚úì Train loader: {len(train_loader)} batches\")\n",
        "print(f\"‚úì Eval loader: {len(eval_loader)} batches\")\n",
        "\n",
        "# Test batch\n",
        "test_batch = next(iter(train_loader))\n",
        "print(f\"\\nüìã Test batch info:\")\n",
        "print(f\"   milp_decisions shape: {test_batch['milp_decisions'].shape}\")\n",
        "print(f\"   embeddings shape: {test_batch['embeddings'].shape}\")\n",
        "print(f\"   zone_mask shape: {test_batch['zone_mask'].shape}\")\n",
        "print(f\"   n_zones: {test_batch['n_zones']}\")\n",
        "print(f\"   max_zones in batch: {test_batch['max_zones']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zCefECXD4YPj"
      },
      "outputs": [],
      "source": [
        "# Training step function with dynamic zone support and LP Oracle\n",
        "def train_step_with_oracle(batch, ebm, sampler, loss_fn, optimizer, config, lp_oracle=None):\n",
        "    \"\"\"\n",
        "    Single training step with dynamic zone counts and optional LP Oracle.\n",
        "    \"\"\"\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    B = len(batch[\"scenario_ids\"])\n",
        "    device = config.device\n",
        "    max_zones = batch[\"max_zones\"]  # Max zones in this batch\n",
        "    n_timesteps = batch[\"n_timesteps\"]\n",
        "\n",
        "    # Move data to device\n",
        "    h = batch[\"embeddings\"].to(device)           # [B, Z_max, T, D]\n",
        "    u_positive = batch[\"milp_decisions\"].to(device)  # [B, Z_max, T, F]\n",
        "    costs_positive = batch[\"milp_objectives\"].to(device)  # [B]\n",
        "    zone_mask = batch[\"zone_mask\"].to(device)    # [B, Z_max]\n",
        "\n",
        "    K = config.num_candidates\n",
        "\n",
        "    # Aggregate embeddings for conditioning (mean over zones and time)\n",
        "    # Apply zone mask before averaging\n",
        "    zone_mask_expanded = zone_mask.unsqueeze(-1).unsqueeze(-1)  # [B, Z_max, 1, 1]\n",
        "    h_masked = h * zone_mask_expanded\n",
        "    h_sum = h_masked.sum(dim=(1, 2))  # [B, D]\n",
        "    h_count = zone_mask.sum(dim=1, keepdim=True) * n_timesteps  # [B, 1]\n",
        "    h_flat = h_sum / h_count.clamp(min=1)  # [B, D]\n",
        "\n",
        "    # Sample candidates using Langevin\n",
        "    u_candidates, _ = sampler.sample(\n",
        "        h=h_flat,\n",
        "        n_samples=K,\n",
        "        n_zones=max_zones,\n",
        "        n_timesteps=n_timesteps,\n",
        "        n_features=config.n_features,\n",
        "    )  # [B, K, Z_max, T, 8]\n",
        "\n",
        "    # Binarize\n",
        "    u_binary = binarize_decisions(u_candidates, method=\"threshold\")\n",
        "\n",
        "    # Evaluate candidates with LP Oracle\n",
        "    if lp_oracle is not None and config.use_lp_worker:\n",
        "        costs_negative = torch.zeros(B, K, device=device)\n",
        "        for b in range(B):\n",
        "            scenario_id = batch[\"scenario_ids\"][b]\n",
        "            n_zones_b = batch[\"n_zones\"][b]  # Original zone count\n",
        "            for k in range(K):\n",
        "                # Use only valid zones for evaluation\n",
        "                u_valid = u_binary[b, k, :n_zones_b].cpu()\n",
        "                result = lp_oracle.evaluate(scenario_id, u_valid)\n",
        "                costs_negative[b, k] = result.objective_value\n",
        "    else:\n",
        "        # Dummy costs based on heuristics\n",
        "        thermal_usage = u_binary[..., 6].sum(dim=(-1, -2, -3))  # [B, K]\n",
        "        costs_negative = 1e6 + thermal_usage * 1e4 + torch.randn(B, K, device=device) * 1e4\n",
        "\n",
        "    # Flatten decisions for EBM\n",
        "    u_pos_flat = u_positive.view(B, -1)  # [B, Z_max * T * F]\n",
        "    energy_positive = ebm(u_pos_flat, h_flat)  # [B]\n",
        "\n",
        "    u_neg_flat = u_binary.view(B * K, -1)  # [B*K, Z_max * T * F]\n",
        "    h_expanded = h_flat.unsqueeze(1).expand(-1, K, -1).reshape(B * K, -1)\n",
        "    energy_negatives = ebm(u_neg_flat, h_expanded).view(B, K)  # [B, K]\n",
        "\n",
        "    # Compute loss\n",
        "    loss, loss_components = loss_fn(\n",
        "        energy_positive=energy_positive,\n",
        "        energy_negatives=energy_negatives,\n",
        "        cost_positive=costs_positive,\n",
        "        costs_negative=costs_negative,\n",
        "    )\n",
        "\n",
        "    # Backward\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient clipping\n",
        "    if config.gradient_clip > 0:\n",
        "        torch.nn.utils.clip_grad_norm_(ebm.parameters(), config.gradient_clip)\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Compute metrics\n",
        "    with torch.no_grad():\n",
        "        cost_gap = (costs_negative - costs_positive.unsqueeze(1)).mean()\n",
        "        energy_gap = (energy_negatives.mean() - energy_positive.mean())\n",
        "\n",
        "    metrics = {\n",
        "        **loss_components,\n",
        "        \"loss\": loss.item(),\n",
        "        \"mean_energy_pos\": energy_positive.mean().item(),\n",
        "        \"mean_energy_neg\": energy_negatives.mean().item(),\n",
        "        \"energy_gap\": energy_gap.item(),\n",
        "        \"mean_cost_gap\": cost_gap.item(),\n",
        "    }\n",
        "\n",
        "    return loss.item(), metrics\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_oracle(eval_loader, ebm, sampler, loss_fn, config, lp_oracle=None):\n",
        "    \"\"\"Evaluate on validation set with dynamic zone support.\"\"\"\n",
        "    ebm.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_energy_pos = 0\n",
        "    total_energy_neg = 0\n",
        "    n_batches = 0\n",
        "\n",
        "    for batch in eval_loader:\n",
        "        B = len(batch[\"scenario_ids\"])\n",
        "        device = config.device\n",
        "        K = config.num_candidates\n",
        "        max_zones = batch[\"max_zones\"]\n",
        "        n_timesteps = batch[\"n_timesteps\"]\n",
        "\n",
        "        h = batch[\"embeddings\"].to(device)\n",
        "        u_positive = batch[\"milp_decisions\"].to(device)\n",
        "        costs_positive = batch[\"milp_objectives\"].to(device)\n",
        "        zone_mask = batch[\"zone_mask\"].to(device)\n",
        "\n",
        "        # Aggregate embeddings\n",
        "        zone_mask_expanded = zone_mask.unsqueeze(-1).unsqueeze(-1)\n",
        "        h_masked = h * zone_mask_expanded\n",
        "        h_sum = h_masked.sum(dim=(1, 2))\n",
        "        h_count = zone_mask.sum(dim=1, keepdim=True) * n_timesteps\n",
        "        h_flat = h_sum / h_count.clamp(min=1)\n",
        "\n",
        "        # Sample candidates\n",
        "        u_candidates, _ = sampler.sample(\n",
        "            h=h_flat,\n",
        "            n_samples=K,\n",
        "            n_zones=max_zones,\n",
        "            n_timesteps=n_timesteps,\n",
        "            n_features=config.n_features,\n",
        "        )\n",
        "        u_binary = binarize_decisions(u_candidates)\n",
        "\n",
        "        # Get costs\n",
        "        if lp_oracle is not None and config.use_lp_worker:\n",
        "            costs_negative = torch.zeros(B, K, device=device)\n",
        "            for b in range(B):\n",
        "                scenario_id = batch[\"scenario_ids\"][b]\n",
        "                n_zones_b = batch[\"n_zones\"][b]\n",
        "                for k in range(K):\n",
        "                    u_valid = u_binary[b, k, :n_zones_b].cpu()\n",
        "                    result = lp_oracle.evaluate(scenario_id, u_valid)\n",
        "                    costs_negative[b, k] = result.objective_value\n",
        "        else:\n",
        "            thermal_usage = u_binary[..., 6].sum(dim=(-1, -2, -3))\n",
        "            costs_negative = 1e6 + thermal_usage * 1e4 + torch.randn(B, K, device=device) * 1e4\n",
        "\n",
        "        # Compute energies\n",
        "        u_pos_flat = u_positive.view(B, -1)\n",
        "        energy_positive = ebm(u_pos_flat, h_flat)\n",
        "\n",
        "        u_neg_flat = u_binary.view(B * K, -1)\n",
        "        h_exp = h_flat.unsqueeze(1).expand(-1, K, -1).reshape(B * K, -1)\n",
        "        energy_negatives = ebm(u_neg_flat, h_exp).view(B, K)\n",
        "\n",
        "        loss, _ = loss_fn(\n",
        "            energy_positive, energy_negatives,\n",
        "            costs_positive, costs_negative\n",
        "        )\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_energy_pos += energy_positive.mean().item()\n",
        "        total_energy_neg += energy_negatives.mean().item()\n",
        "        n_batches += 1\n",
        "\n",
        "    ebm.train()\n",
        "\n",
        "    return {\n",
        "        \"val_loss\": total_loss / max(1, n_batches),\n",
        "        \"val_energy_pos\": total_energy_pos / max(1, n_batches),\n",
        "        \"val_energy_neg\": total_energy_neg / max(1, n_batches),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# OPTIONAL: LOAD CHECKPOINT TO RESUME TRAINING\n",
        "# ============================================================================\n",
        "# Set RESUME_TRAINING = True to continue from a saved checkpoint\n",
        "# Set CHECKPOINT_PATH to the path of the checkpoint file\n",
        "# ============================================================================\n",
        "\n",
        "RESUME_TRAINING = False  # ‚Üê Change to True to resume\n",
        "CHECKPOINT_PATH = None   # ‚Üê Set path, e.g., \"outputs/preference_training/best_model.pt\"\n",
        "                         # or \"outputs/preference_training/checkpoint_phase2_epoch_5.pt\"\n",
        "SKIP_PHASE1 = False      # ‚Üê Set True to skip Phase 1 entirely (if already done)\n",
        "START_PHASE2_EPOCH = 1   # ‚Üê Set starting epoch for Phase 2 (if resuming mid-phase)\n",
        "\n",
        "if RESUME_TRAINING and CHECKPOINT_PATH is not None:\n",
        "    checkpoint_file = Path(CHECKPOINT_PATH)\n",
        "    \n",
        "    if checkpoint_file.exists():\n",
        "        print(f\"üìÇ Loading checkpoint from: {checkpoint_file}\")\n",
        "        checkpoint = torch.load(checkpoint_file, map_location=config.device)\n",
        "        \n",
        "        # Load model weights\n",
        "        if 'model_state_dict' in checkpoint:\n",
        "            ebm.load_state_dict(checkpoint['model_state_dict'])\n",
        "            print(f\"   ‚úì EBM model weights loaded\")\n",
        "        \n",
        "        # Load conditioner weights if available\n",
        "        if 'conditioner_state_dict' in checkpoint and checkpoint['conditioner_state_dict'] is not None:\n",
        "            if trainer.conditioner is not None:\n",
        "                trainer.conditioner.load_state_dict(checkpoint['conditioner_state_dict'])\n",
        "                print(f\"   ‚úì HConditioner weights loaded\")\n",
        "        \n",
        "        # Load cost proxy weights if available\n",
        "        if 'cost_proxy_state_dict' in checkpoint and checkpoint['cost_proxy_state_dict'] is not None:\n",
        "            if trainer.cost_proxy is not None:\n",
        "                trainer.cost_proxy.load_state_dict(checkpoint['cost_proxy_state_dict'])\n",
        "                print(f\"   ‚úì CostProxy weights loaded\")\n",
        "        \n",
        "        # Load history if available\n",
        "        if 'history' in checkpoint:\n",
        "            for k, v in checkpoint['history'].items():\n",
        "                history[k] = v\n",
        "            print(f\"   ‚úì Training history loaded ({len(history)} keys)\")\n",
        "        \n",
        "        # Get checkpoint info\n",
        "        ckpt_epoch = checkpoint.get('epoch', 0)\n",
        "        ckpt_phase = checkpoint.get('phase', 'unknown')\n",
        "        ckpt_loss = checkpoint.get('loss', float('inf'))\n",
        "        \n",
        "        print(f\"\\nüìä Checkpoint info:\")\n",
        "        print(f\"   Phase: {ckpt_phase}\")\n",
        "        print(f\"   Epoch: {ckpt_epoch}\")\n",
        "        print(f\"   Loss: {ckpt_loss:.4f}\")\n",
        "        \n",
        "        # Auto-configure resume settings\n",
        "        if ckpt_phase == 'finetune':\n",
        "            SKIP_PHASE1 = True\n",
        "            START_PHASE2_EPOCH = ckpt_epoch + 1\n",
        "            best_loss = min(history.get('phase1_loss', [float('inf')]))\n",
        "            print(f\"\\n‚öôÔ∏è Auto-configured:\")\n",
        "            print(f\"   SKIP_PHASE1 = True (Phase 1 already done)\")\n",
        "            print(f\"   START_PHASE2_EPOCH = {START_PHASE2_EPOCH}\")\n",
        "        elif ckpt_phase == 'pretrain':\n",
        "            SKIP_PHASE1 = True  # Phase 1 was completed\n",
        "            best_loss = ckpt_loss\n",
        "            print(f\"\\n‚öôÔ∏è Auto-configured:\")\n",
        "            print(f\"   SKIP_PHASE1 = True (will start Phase 2)\")\n",
        "        \n",
        "        print(f\"\\n‚úÖ Checkpoint loaded successfully!\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Checkpoint file not found: {checkpoint_file}\")\n",
        "        print(f\"   Training will start from scratch.\")\n",
        "        RESUME_TRAINING = False\n",
        "else:\n",
        "    if RESUME_TRAINING:\n",
        "        print(\"‚ö†Ô∏è RESUME_TRAINING=True but CHECKPOINT_PATH is None\")\n",
        "        print(\"   Please set CHECKPOINT_PATH to resume training.\")\n",
        "    print(\"üÜï Starting fresh training (no checkpoint loaded)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711,
          "referenced_widgets": [
            "9f34a6f58b504d0291e2543a9781dc45",
            "186c5363609d48099066f542bacfcaae",
            "5728ae23ecd04b4fbd251e29d0b4923c",
            "e3e22e5a453b4d669611fd18f8dcd6f4",
            "507b8388f4804f40a1d56e4c350d10e5",
            "863a80c645c34101a12d17d8866d6f74",
            "b1ad73501fa0471dbd847d9474a6d209",
            "eece68bd0bfa4598b1404638ec218245",
            "90797683e0dd44e1871bfd5ac53d14e5",
            "dc04af65558f4142891b33eab48c2839",
            "aa2dc7da3efd455eab9153b88492a218"
          ]
        },
        "id": "8i9xjZut0lp8",
        "outputId": "1217dbb2-f93c-47a3-d065-a405776a340a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PREFERENCE-BASED EBM TRAINING\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Epochs: 50\n",
            "Architecture: gru\n",
            "Candidates per scenario: 3\n",
            "LP Oracle: Enabled\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f34a6f58b504d0291e2543a9781dc45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/50:   0%|          | 0/1184 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì LPWorkerTwoStage initialized\n",
            "  Solver: appsi_highs\n",
            "  Slack tolerance: 1.0 MWh (dt=1.0h)\n",
            "  Deviation penalty (Œª): 10000.0\n",
            "  Flip budgets: K=20‚Üí100, K=100‚Üí1000, full_soft‚ÜíNone\n",
            "  Time limits: TL1=0.3s, TL2=0.2s, TL3=0.2s, TL4=0.5s\n",
            "‚úì LPOracle initialized with LPWorkerTwoStage\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2106484444.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Use the oracle-aware training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         loss, metrics = train_step_with_oracle(\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlp_oracle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         )\n",
            "\u001b[0;32m/tmp/ipython-input-1523417414.py\u001b[0m in \u001b[0;36mtrain_step_with_oracle\u001b[0;34m(batch, ebm, sampler, loss_fn, optimizer, config, lp_oracle)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;31m# Use only valid zones for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mu_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu_binary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_zones_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlp_oracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mcosts_negative\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/benchmark/src/preference/lp_oracle.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, scenario_id, u_zt)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mOracleResult\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeasibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# Ensure binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/benchmark/src/milp/lp_worker_two_stage.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, scenario_id, decoder_output)\u001b[0m\n\u001b[1;32m   1009\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_flip_budget_constraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritical_20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflip_budget_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_all_binaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mres2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve_with_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_limits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'repair_20'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_repair_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/benchmark/src/milp/lp_worker_two_stage.py\u001b[0m in \u001b[0;36m_solve_with_timeout\u001b[0;34m(self, model, time_limit)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seconds'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtee\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_solutions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyomo/contrib/appsi/base.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, model, tee, load_solutions, logfile, solnfile, timelimit, report_timing, solver_io, suffixes, options, keepfiles, symbolic_solver_labels, warmstart)\u001b[0m\n\u001b[1;32m   1566\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mResults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacySolverInterface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0mlegacy_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLegacySolverResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyomo/contrib/appsi/solvers/highs.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, model, timer)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'set_instance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'set_instance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyomo/contrib/appsi/solvers/highs.py\u001b[0m in \u001b[0;36mset_instance\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solver_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhighspy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHighs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objective\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyomo/contrib/appsi/base.py\u001b[0m in \u001b[0;36madd_block\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1137\u001b[0m                 )\n\u001b[1;32m   1138\u001b[0m             )\n\u001b[0;32m-> 1139\u001b[0;31m         self.add_constraints(\n\u001b[0m\u001b[1;32m   1140\u001b[0m             [\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mcon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyomo/contrib/appsi/base.py\u001b[0m in \u001b[0;36madd_constraints\u001b[0;34m(self, cons)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0mnamed_exprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexternal_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_only_child_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_for_new_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_named_expressions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_exprs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexternal_functions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyomo/contrib/appsi/base.py\u001b[0m in \u001b[0;36m_check_for_new_vars\u001b[0;34m(self, variables)\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mv_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_referenced_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m                 \u001b[0mnew_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_to_remove_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVarData\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyomo/contrib/appsi/base.py\u001b[0m in \u001b[0;36madd_variables\u001b[0;34m(self, variables)\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             )\n\u001b[0;32m-> 1000\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyomo/contrib/appsi/solvers/highs.py\u001b[0m in \u001b[0;36m_add_variables\u001b[0;34m(self, variables)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mubs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         )\n\u001b[0;32m--> 370\u001b[0;31m         self._solver_model.changeColsIntegrality(\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# TWO-PHASE TRAINING LOOP (Production-Grade v2)\n",
        "# ============================================================================\n",
        "# Phase 1: Contrastive (InfoNCE) without LP - stable energy geometry\n",
        "# Phase 2: Ranked (WeightedMarginRankingLoss) with LP - cost-aware\n",
        "# ============================================================================\n",
        "\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from src.preference.training_strategy import (\n",
        "    TwoPhaseConfig,\n",
        "    TwoPhaseTrainer,\n",
        "    TrainingPhase,\n",
        "    create_two_phase_trainer,\n",
        ")\n",
        "from src.preference.conditioning import (\n",
        "    HConditioner,\n",
        "    DecisionFeatureExtractor,\n",
        "    FeatureBasedCostProxy,\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# EARLY STOPPING HELPER\n",
        "# ============================================================================\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int = 3, min_delta: float = 5e-4):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.should_stop = False\n",
        "    \n",
        "    def step(self, loss: float) -> bool:\n",
        "        if loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.should_stop = True\n",
        "        return self.should_stop\n",
        "\n",
        "# ============================================================================\n",
        "# PHASE 1 SCHEDULE: Adaptive K and steps\n",
        "# ============================================================================\n",
        "def get_phase1_schedule(epoch: int):\n",
        "    \"\"\"\n",
        "    Returns (K, langevin_steps) for Phase 1 based on epoch.\n",
        "    epochs 1-3:  K=3, steps=10\n",
        "    epochs 4-8:  K=6, steps=15\n",
        "    epochs 9-12: K=8, steps=20\n",
        "    \"\"\"\n",
        "    if epoch <= 3:\n",
        "        return 3, 10\n",
        "    elif epoch <= 8:\n",
        "        return 6, 15\n",
        "    else:\n",
        "        return 8, 20\n",
        "\n",
        "# Create TwoPhaseConfig from our config\n",
        "two_phase_config = TwoPhaseConfig(\n",
        "    # Phase 1: Pretrain (contrastive) - will be overridden by schedule\n",
        "    pretrain_epochs=12,  # max epochs (early stopping may end sooner)\n",
        "    pretrain_lr=config.pretrain_lr,\n",
        "    pretrain_langevin_steps=10,  # base, will be updated per epoch\n",
        "    pretrain_num_candidates=3,   # base, will be updated per epoch\n",
        "    \n",
        "    # Phase 2: Finetune (ranked with LP)\n",
        "    finetune_epochs=config.finetune_epochs,\n",
        "    finetune_lr=config.finetune_lr,\n",
        "    finetune_langevin_steps=config.finetune_langevin_steps,\n",
        "    finetune_lp_ratio=config.finetune_lp_ratio,\n",
        "    finetune_num_candidates_min=config.finetune_num_candidates_min,\n",
        "    finetune_num_candidates_max=config.finetune_num_candidates_max,\n",
        "    finetune_uncertainty_top=config.finetune_uncertainty_top,\n",
        "    finetune_cost_proxy_top_k=config.finetune_cost_proxy_top_k,\n",
        "    \n",
        "    # General\n",
        "    batch_size=config.batch_size,\n",
        "    margin=config.margin,\n",
        "    device=config.device,\n",
        ")\n",
        "\n",
        "# Create two-phase trainer with new architecture\n",
        "trainer = create_two_phase_trainer(\n",
        "    ebm=ebm,\n",
        "    sampler=sampler,\n",
        "    config=two_phase_config,\n",
        "    lp_oracle=lp_oracle if config.use_lp_worker else None,\n",
        "    embedding_dim=config.h_dim,\n",
        "    use_cost_proxy=True,\n",
        "    use_conditioner=True,\n",
        "    use_decoder=True,  # FeasibilityDecoder for mutual exclusion\n",
        ")\n",
        "\n",
        "print(\"‚úì TwoPhaseTrainer initialized (v2)\")\n",
        "print(f\"  HConditioner: {'Enabled' if trainer.conditioner else 'Disabled'}\")\n",
        "print(f\"  FeatureBasedCostProxy: {'Enabled' if trainer.cost_proxy else 'Disabled'}\")\n",
        "print(f\"  FeasibilityDecoder: {'Enabled' if trainer.decoder else 'Disabled'}\")\n",
        "print(f\"  LP Oracle: {'Enabled' if config.use_lp_worker else 'Disabled'}\")\n",
        "\n",
        "# Training history (may be pre-loaded from checkpoint)\n",
        "if 'history' not in dir() or not history:\n",
        "    history = defaultdict(list)\n",
        "if 'best_loss' not in dir():\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ TWO-PHASE PREFERENCE-BASED EBM TRAINING (v2)\")\n",
        "print(\"=\" * 70)\n",
        "print(\"   Phase 1: InfoNCE contrastive + hard negatives (early stopping)\")\n",
        "print(\"   Phase 2: WeightedMarginRankingLoss + FeasibilityDecoder (cost-aware)\")\n",
        "if RESUME_TRAINING:\n",
        "    print(f\"   üìÇ RESUMING FROM CHECKPOINT\")\n",
        "    print(f\"      Skip Phase 1: {SKIP_PHASE1}\")\n",
        "    print(f\"      Start Phase 2 epoch: {START_PHASE2_EPOCH}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# ============================================================================\n",
        "# PHASE 1: PRETRAIN - Contrastive with Early Stopping & Adaptive Schedule\n",
        "# ============================================================================\n",
        "if not SKIP_PHASE1:\n",
        "    phase1_config = two_phase_config.get_phase_config(TrainingPhase.PRETRAIN)\n",
        "    phase1_max_epochs = 12\n",
        "\n",
        "    optimizer_phase1 = torch.optim.AdamW(\n",
        "        list(ebm.parameters()) + (list(trainer.conditioner.parameters()) if trainer.conditioner else []),\n",
        "        lr=phase1_config.learning_rate,\n",
        "        weight_decay=config.weight_decay,\n",
        "    )\n",
        "    scheduler_phase1 = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer_phase1, T_max=phase1_max_epochs, eta_min=phase1_config.learning_rate * 0.01\n",
        "    )\n",
        "\n",
        "    early_stopper = EarlyStopping(patience=3, min_delta=5e-4)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"üì¶ PHASE 1: PRETRAIN (Contrastive + Hard Negatives)\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Loss: InfoNCE + top-2 hard negatives\")\n",
        "    print(f\"   Max epochs: {phase1_max_epochs} (early stopping: patience=3)\")\n",
        "    print(f\"   LP Oracle: DISABLED\")\n",
        "    print(f\"   FeasibilityDecoder: DISABLED (Phase 1)\")\n",
        "    print(f\"   Schedule:\")\n",
        "    print(f\"     epochs 1-3:  K=3, steps=10\")\n",
        "    print(f\"     epochs 4-8:  K=6, steps=15\")\n",
        "    print(f\"     epochs 9-12: K=8, steps=20\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    ebm.train()\n",
        "    if trainer.conditioner:\n",
        "        trainer.conditioner.train()\n",
        "\n",
        "    for epoch in range(1, phase1_max_epochs + 1):\n",
        "        epoch_start = time.time()\n",
        "        epoch_losses = []\n",
        "        epoch_energy_pos = []\n",
        "        epoch_energy_neg = []\n",
        "        epoch_energy_gap = []\n",
        "        \n",
        "        # Get adaptive K and steps for this epoch\n",
        "        K_epoch, steps_epoch = get_phase1_schedule(epoch)\n",
        "        phase1_config.num_candidates_min = K_epoch\n",
        "        phase1_config.num_candidates_max = K_epoch\n",
        "        phase1_config.langevin_steps = steps_epoch\n",
        "        \n",
        "        pbar = tqdm(train_loader, desc=f\"[P1] Epoch {epoch}/{phase1_max_epochs} (K={K_epoch})\", leave=False)\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            metrics = trainer.train_step(\n",
        "                batch, optimizer_phase1, phase1_config, epoch, batch_idx\n",
        "            )\n",
        "            epoch_losses.append(metrics['loss'])\n",
        "            epoch_energy_pos.append(metrics.get('energy_pos', 0))\n",
        "            epoch_energy_neg.append(metrics.get('energy_neg', 0))\n",
        "            epoch_energy_gap.append(metrics.get('energy_gap', 0))\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                \"loss\": f\"{metrics['loss']:.4f}\",\n",
        "                \"E+\": f\"{metrics.get('energy_pos', 0):.2f}\",\n",
        "                \"E-\": f\"{metrics.get('energy_neg', 0):.2f}\",\n",
        "                \"gap\": f\"{metrics.get('energy_gap', 0):.2f}\",\n",
        "            })\n",
        "        \n",
        "        scheduler_phase1.step()\n",
        "        \n",
        "        avg_loss = np.mean(epoch_losses)\n",
        "        avg_e_pos = np.mean(epoch_energy_pos)\n",
        "        avg_e_neg = np.mean(epoch_energy_neg)\n",
        "        avg_gap = np.mean(epoch_energy_gap)\n",
        "        \n",
        "        history[\"phase1_loss\"].append(avg_loss)\n",
        "        history[\"phase1_energy_pos\"].append(avg_e_pos)\n",
        "        history[\"phase1_energy_neg\"].append(avg_e_neg)\n",
        "        history[\"phase1_energy_gap\"].append(avg_gap)\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        \n",
        "        # Early stopping check\n",
        "        should_stop = early_stopper.step(avg_loss)\n",
        "        stop_indicator = \" ‚ö†Ô∏è EARLY STOP\" if should_stop else \"\"\n",
        "        \n",
        "        print(f\"[P1] Epoch {epoch}/{phase1_max_epochs} ({epoch_time:.1f}s) K={K_epoch} | \"\n",
        "              f\"Loss: {avg_loss:.4f} | E+: {avg_e_pos:.3f} | E-: {avg_e_neg:.3f} | \"\n",
        "              f\"Gap: {avg_gap:.3f} | LR: {scheduler_phase1.get_last_lr()[0]:.2e}{stop_indicator}\")\n",
        "        \n",
        "        # Save best\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save({\n",
        "                \"epoch\": epoch, \"phase\": \"pretrain\",\n",
        "                \"model_state_dict\": ebm.state_dict(),\n",
        "                \"conditioner_state_dict\": trainer.conditioner.state_dict() if trainer.conditioner else None,\n",
        "                \"loss\": best_loss,\n",
        "            }, Path(config.output_dir) / \"best_model_phase1.pt\")\n",
        "        \n",
        "        if should_stop:\n",
        "            print(f\"   Early stopping triggered after {epoch} epochs (no improvement for {early_stopper.patience} epochs)\")\n",
        "            break\n",
        "\n",
        "    phase1_time = time.time() - start_time\n",
        "    print(f\"\\n‚úì Phase 1 complete: {phase1_time/60:.1f} min, Best loss: {best_loss:.4f}\")\n",
        "else:\n",
        "    print(f\"\\n‚è≠Ô∏è SKIPPING PHASE 1 (already completed, loading from checkpoint)\")\n",
        "    phase1_time = 0\n",
        "\n",
        "# ============================================================================\n",
        "# PHASE 2: FINETUNE - Ranked with LP + FeasibilityDecoder (detailed logging)\n",
        "# ============================================================================\n",
        "phase2_config = two_phase_config.get_phase_config(TrainingPhase.FINETUNE)\n",
        "optimizer_phase2 = torch.optim.AdamW(\n",
        "    list(ebm.parameters()) + \n",
        "    (list(trainer.conditioner.parameters()) if trainer.conditioner else []) +\n",
        "    (list(trainer.cost_proxy.parameters()) if trainer.cost_proxy else []),\n",
        "    lr=phase2_config.learning_rate,\n",
        "    weight_decay=config.weight_decay,\n",
        ")\n",
        "scheduler_phase2 = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_phase2, T_max=phase2_config.epochs, eta_min=phase2_config.learning_rate * 0.01\n",
        ")\n",
        "\n",
        "# If resuming, step scheduler to correct position\n",
        "if RESUME_TRAINING and START_PHASE2_EPOCH > 1:\n",
        "    for _ in range(START_PHASE2_EPOCH - 1):\n",
        "        scheduler_phase2.step()\n",
        "    print(f\"   Scheduler stepped to epoch {START_PHASE2_EPOCH}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"üéØ PHASE 2: FINETUNE (Ranked with LP + FeasibilityDecoder)\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"   Loss: WeightedMarginRankingLoss (cost-aware)\")\n",
        "print(f\"   Epochs: {START_PHASE2_EPOCH} ‚Üí {phase2_config.epochs}\")\n",
        "print(f\"   LP Oracle: {phase2_config.lp_batch_ratio:.0%} of batches\")\n",
        "print(f\"   FeasibilityDecoder: ENABLED (mutual exclusion)\")\n",
        "print(f\"   Uncertainty filter: top {phase2_config.uncertainty_threshold:.0%}\")\n",
        "print(f\"   Candidates: K={phase2_config.num_candidates_min}‚Üí{phase2_config.num_candidates_max}\")\n",
        "print(f\"   Cost proxy filter: top-{phase2_config.cost_proxy_top_k}\")\n",
        "print(f\"   Langevin steps: {phase2_config.langevin_steps}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "phase2_start = time.time()\n",
        "best_loss_phase2 = float(\"inf\")\n",
        "\n",
        "# Load best_loss_phase2 from history if resuming\n",
        "if RESUME_TRAINING and history.get('phase2_loss'):\n",
        "    best_loss_phase2 = min(history['phase2_loss'])\n",
        "    print(f\"   Best Phase 2 loss from history: {best_loss_phase2:.4f}\")\n",
        "\n",
        "for epoch in range(START_PHASE2_EPOCH, phase2_config.epochs + 1):\n",
        "    epoch_start = time.time()\n",
        "    epoch_losses = []\n",
        "    epoch_energy_pos = []\n",
        "    epoch_energy_neg = []\n",
        "    epoch_energy_gap = []\n",
        "    epoch_lp_ratios = []\n",
        "    epoch_loss_rank = []\n",
        "    epoch_mean_weight = []\n",
        "    epoch_cost_gap = []\n",
        "    epoch_cost_gap_p90 = []\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f\"[P2] Epoch {epoch}/{phase2_config.epochs}\", leave=False)\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        metrics = trainer.train_step(\n",
        "            batch, optimizer_phase2, phase2_config, epoch, batch_idx\n",
        "        )\n",
        "        epoch_losses.append(metrics['loss'])\n",
        "        epoch_energy_pos.append(metrics.get('energy_pos', 0))\n",
        "        epoch_energy_neg.append(metrics.get('energy_neg', 0))\n",
        "        epoch_energy_gap.append(metrics.get('energy_gap', 0))\n",
        "        epoch_lp_ratios.append(metrics.get('lp_ratio', 0))\n",
        "        epoch_loss_rank.append(metrics.get('loss_rank', 0))\n",
        "        epoch_mean_weight.append(metrics.get('mean_weight', 0))\n",
        "        epoch_cost_gap.append(metrics.get('mean_cost_gap', 0))\n",
        "        epoch_cost_gap_p90.append(metrics.get('cost_gap_p90', 0))\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            \"loss\": f\"{metrics['loss']:.4f}\",\n",
        "            \"L_rank\": f\"{metrics.get('loss_rank', 0):.3f}\",\n",
        "            \"LP\": f\"{metrics.get('lp_ratio', 0):.0%}\",\n",
        "            \"wÃÑ\": f\"{metrics.get('mean_weight', 0):.2f}\",\n",
        "        })\n",
        "    \n",
        "    scheduler_phase2.step()\n",
        "    \n",
        "    avg_loss = np.mean(epoch_losses)\n",
        "    avg_e_pos = np.mean(epoch_energy_pos)\n",
        "    avg_e_neg = np.mean(epoch_energy_neg)\n",
        "    avg_gap = np.mean(epoch_energy_gap)\n",
        "    avg_lp_ratio = np.mean(epoch_lp_ratios)\n",
        "    avg_loss_rank = np.mean(epoch_loss_rank)\n",
        "    avg_mean_weight = np.mean(epoch_mean_weight)\n",
        "    avg_cost_gap = np.mean(epoch_cost_gap)\n",
        "    avg_cost_gap_p90 = np.mean(epoch_cost_gap_p90)\n",
        "    \n",
        "    history[\"phase2_loss\"].append(avg_loss)\n",
        "    history[\"phase2_energy_pos\"].append(avg_e_pos)\n",
        "    history[\"phase2_energy_neg\"].append(avg_e_neg)\n",
        "    history[\"phase2_energy_gap\"].append(avg_gap)\n",
        "    history[\"phase2_lp_ratio\"].append(avg_lp_ratio)\n",
        "    history[\"phase2_loss_rank\"].append(avg_loss_rank)\n",
        "    history[\"phase2_mean_weight\"].append(avg_mean_weight)\n",
        "    history[\"phase2_cost_gap\"].append(avg_cost_gap)\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start\n",
        "    \n",
        "    if epoch % config.log_every == 0:\n",
        "        print(f\"[P2] Epoch {epoch}/{phase2_config.epochs} ({epoch_time:.1f}s)\")\n",
        "        print(f\"     Loss: {avg_loss:.4f} | L_rank: {avg_loss_rank:.4f}\")\n",
        "        print(f\"     E+: {avg_e_pos:.3f} | E-: {avg_e_neg:.3f} | Gap: {avg_gap:.3f}\")\n",
        "        print(f\"     LP%: {avg_lp_ratio:.1%} | wÃÑ: {avg_mean_weight:.2f}\")\n",
        "        print(f\"     Cost gap: {avg_cost_gap:.1f} | p90: {avg_cost_gap_p90:.1f}\")\n",
        "    \n",
        "    # Evaluation\n",
        "    if epoch % config.eval_every == 0 and len(eval_loader) > 0:\n",
        "        val_metrics = evaluate_with_oracle(eval_loader, ebm, sampler, loss_fn, config, lp_oracle)\n",
        "        history[\"val_loss\"].append(val_metrics[\"val_loss\"])\n",
        "        print(f\"     Val Loss: {val_metrics['val_loss']:.4f}\")\n",
        "    \n",
        "    # Save best\n",
        "    if avg_loss < best_loss_phase2:\n",
        "        best_loss_phase2 = avg_loss\n",
        "        torch.save({\n",
        "            \"epoch\": epoch, \"phase\": \"finetune\",\n",
        "            \"model_state_dict\": ebm.state_dict(),\n",
        "            \"conditioner_state_dict\": trainer.conditioner.state_dict() if trainer.conditioner else None,\n",
        "            \"cost_proxy_state_dict\": trainer.cost_proxy.state_dict() if trainer.cost_proxy else None,\n",
        "            \"optimizer_state_dict\": optimizer_phase2.state_dict(),\n",
        "            \"loss\": best_loss_phase2,\n",
        "            \"config\": config.__dict__,\n",
        "        }, Path(config.output_dir) / \"best_model.pt\")\n",
        "    \n",
        "    # Checkpoint every save_every epochs\n",
        "    if epoch % config.save_every == 0:\n",
        "        torch.save({\n",
        "            \"epoch\": epoch, \"phase\": \"finetune\",\n",
        "            \"model_state_dict\": ebm.state_dict(),\n",
        "            \"conditioner_state_dict\": trainer.conditioner.state_dict() if trainer.conditioner else None,\n",
        "            \"cost_proxy_state_dict\": trainer.cost_proxy.state_dict() if trainer.cost_proxy else None,\n",
        "            \"history\": dict(history),\n",
        "            \"trainer_stats\": trainer.get_statistics(),\n",
        "        }, Path(config.output_dir) / f\"checkpoint_phase2_epoch_{epoch}.pt\")\n",
        "        print(f\"     üíæ Checkpoint saved: checkpoint_phase2_epoch_{epoch}.pt\")\n",
        "\n",
        "phase2_time = time.time() - phase2_start\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "# Final statistics\n",
        "stats = trainer.get_statistics()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ TWO-PHASE TRAINING COMPLETE (v2)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üìä Total time: {total_time/60:.1f} minutes\")\n",
        "print(f\"   Phase 1 (contrastive): {phase1_time/60:.1f} min ({len(history.get('phase1_loss', []))} epochs)\")\n",
        "print(f\"   Phase 2 (ranked): {phase2_time/60:.1f} min\")\n",
        "print(f\"\\nüìà Best losses:\")\n",
        "print(f\"   Phase 1 (InfoNCE): {best_loss:.4f}\")\n",
        "print(f\"   Phase 2 (Ranked): {best_loss_phase2:.4f}\")\n",
        "print(f\"\\n‚ö° Final energies:\")\n",
        "if history.get('phase2_energy_pos'):\n",
        "    print(f\"   E+ (positive): {history['phase2_energy_pos'][-1]:.3f}\")\n",
        "    print(f\"   E- (negative): {history['phase2_energy_neg'][-1]:.3f}\")\n",
        "    print(f\"   Gap (E- - E+): {history['phase2_energy_gap'][-1]:.3f}\")\n",
        "print(f\"\\nüî¨ LP Oracle statistics:\")\n",
        "print(f\"   Total LP calls: {stats['lp_calls']:,}\")\n",
        "print(f\"   LP skipped: {stats['lp_skipped']:,}\")\n",
        "print(f\"   Effective LP ratio: {stats['lp_ratio']:.1%}\")\n",
        "print(f\"   Proxy filtered: {stats['proxy_filtered']:,}\")\n",
        "print(f\"\\nüíæ Model saved to: {config.output_dir}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-wA8zkq0lp8"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 6. EVALUATION & VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Training Loss\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(history[\"train_loss\"], label=\"Train Loss\", linewidth=2)\n",
        "if \"val_loss\" in history and len(history[\"val_loss\"]) > 0:\n",
        "    eval_epochs = list(range(config.eval_every, len(history[\"train_loss\"])+1, config.eval_every))\n",
        "    ax1.plot(eval_epochs, history[\"val_loss\"], label=\"Val Loss\", linewidth=2, marker='o')\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax1.set_title(\"Training & Validation Loss\")\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Energy Gap (E- - E+)\n",
        "ax2 = axes[0, 1]\n",
        "if \"train_energy_gap\" in history:\n",
        "    ax2.plot(history[\"train_energy_gap\"], label=\"Energy Gap (E- - E+)\", linewidth=2, color=\"green\")\n",
        "ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5, label=\"Zero gap\")\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.set_ylabel(\"Energy Gap\")\n",
        "ax2.set_title(\"Energy Gap Evolution (should be positive)\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Mean Energies\n",
        "ax3 = axes[1, 0]\n",
        "if \"train_mean_energy_pos\" in history:\n",
        "    ax3.plot(history[\"train_mean_energy_pos\"], label=\"E(u+ | h) Positive\", linewidth=2)\n",
        "if \"train_mean_energy_neg\" in history:\n",
        "    ax3.plot(history[\"train_mean_energy_neg\"], label=\"E(u- | h) Negative\", linewidth=2)\n",
        "ax3.set_xlabel(\"Epoch\")\n",
        "ax3.set_ylabel(\"Mean Energy\")\n",
        "ax3.set_title(\"Mean Energy by Decision Type\")\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Learning Rate\n",
        "ax4 = axes[1, 1]\n",
        "if \"train_loss\" in history:\n",
        "    lrs = [config.learning_rate * (0.01 + 0.99 * (1 + np.cos(np.pi * i / config.epochs)) / 2)\n",
        "           for i in range(len(history[\"train_loss\"]))]\n",
        "    ax4.plot(lrs, label=\"Learning Rate\", linewidth=2, color=\"purple\")\n",
        "ax4.set_xlabel(\"Epoch\")\n",
        "ax4.set_ylabel(\"Learning Rate\")\n",
        "ax4.set_title(\"Learning Rate Schedule\")\n",
        "ax4.set_yscale('log')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(Path(config.output_dir) / \"training_curves.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úì Training curves saved to: {config.output_dir}/training_curves.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zKYnuLb0lp8"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 7. MODEL ANALYSIS & INFERENCE\n",
        "# ============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_and_rank(ebm, sampler, h, n_candidates, n_zones, n_timesteps, n_features=8):\n",
        "    \"\"\"\n",
        "    Sample candidates and rank by energy.\n",
        "    Returns candidates sorted by energy (lowest first = best).\n",
        "    \"\"\"\n",
        "    ebm.eval()\n",
        "    device = h.device\n",
        "    B = h.shape[0]\n",
        "\n",
        "    # Sample candidates\n",
        "    u_candidates, _ = sampler.sample(\n",
        "        h=h,\n",
        "        n_samples=n_candidates,\n",
        "        n_zones=n_zones,\n",
        "        n_timesteps=n_timesteps,\n",
        "        n_features=n_features,\n",
        "    )  # [B, K, Z, T, 8]\n",
        "\n",
        "    # Compute energies for all candidates\n",
        "    K = n_candidates\n",
        "    u_flat = u_candidates.view(B * K, -1)\n",
        "    h_exp = h.unsqueeze(1).expand(-1, K, -1).reshape(B * K, -1)\n",
        "    energies = ebm(u_flat, h_exp).view(B, K)  # [B, K]\n",
        "\n",
        "    # Sort by energy (ascending)\n",
        "    sorted_indices = energies.argsort(dim=1)\n",
        "\n",
        "    # Reorder candidates\n",
        "    sorted_candidates = torch.gather(\n",
        "        u_candidates,\n",
        "        dim=1,\n",
        "        index=sorted_indices.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(-1, -1, n_zones, n_timesteps, n_features)\n",
        "    )\n",
        "    sorted_energies = torch.gather(energies, dim=1, index=sorted_indices)\n",
        "\n",
        "    return sorted_candidates, sorted_energies\n",
        "\n",
        "# Test inference on a sample\n",
        "if len(eval_dataset) > 0:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"INFERENCE TEST\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Get a sample\n",
        "    sample = eval_dataset[0]\n",
        "    h_test = sample[\"embedding\"].unsqueeze(0).to(config.device)\n",
        "\n",
        "    # Sample and rank candidates\n",
        "    candidates, energies = sample_and_rank(\n",
        "        ebm, sampler, h_test,\n",
        "        n_candidates=10,\n",
        "        n_zones=config.n_zones,\n",
        "        n_timesteps=config.n_timesteps,\n",
        "    )\n",
        "\n",
        "    print(f\"Scenario: {sample['scenario_id']}\")\n",
        "    print(f\"MILP Objective: {sample['milp_objective']:.0f}\")\n",
        "    print(f\"\\nGenerated {candidates.shape[1]} candidates, ranked by energy:\")\n",
        "    for k in range(min(5, candidates.shape[1])):\n",
        "        u_k = candidates[0, k]  # [Z, T, 8]\n",
        "        thermal_on = u_k[..., 6].sum().item()\n",
        "        nuclear_on = u_k[..., 5].sum().item()\n",
        "        print(f\"  Rank {k+1}: E={energies[0, k].item():.3f}, \"\n",
        "              f\"Thermal={thermal_on:.0f}, Nuclear={nuclear_on:.0f}\")\n",
        "\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQLO8vVj0lp8"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 8. SAVE RESULTS & NEXT STEPS\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "\n",
        "# Save training history\n",
        "history_path = Path(config.output_dir) / \"training_history.json\"\n",
        "with open(history_path, 'w') as f:\n",
        "    json.dump({k: [float(v) for v in vals] for k, vals in history.items()}, f, indent=2)\n",
        "print(f\"‚úì Training history saved to: {history_path}\")\n",
        "\n",
        "# Save final model\n",
        "final_path = Path(config.output_dir) / \"final_model.pt\"\n",
        "torch.save({\n",
        "    \"epoch\": config.epochs,\n",
        "    \"model_state_dict\": ebm.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "    \"config\": config.__dict__,\n",
        "    \"history\": dict(history),\n",
        "}, final_path)\n",
        "print(f\"‚úì Final model saved to: {final_path}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Architecture: {config.architecture}\")\n",
        "print(f\"Total epochs: {config.epochs}\")\n",
        "print(f\"Best loss: {best_loss:.4f}\")\n",
        "print(f\"Final train loss: {history['train_loss'][-1]:.4f}\")\n",
        "if \"val_loss\" in history and len(history[\"val_loss\"]) > 0:\n",
        "    print(f\"Final val loss: {history['val_loss'][-1]:.4f}\")\n",
        "print(f\"\\nOutput directory: {config.output_dir}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìã NEXT STEPS:\")\n",
        "print(\"1. Enable LP Worker (set config.use_lp_worker = True) for real cost evaluation\")\n",
        "print(\"2. Increase training scenarios and epochs\")\n",
        "print(\"3. Load HTE embeddings (set config.embeddings_path)\")\n",
        "print(\"4. Run inference on new scenarios and evaluate vs MILP\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVlgdfW_0lp9"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 9. VERIFY EBM LEARNED CORRECTLY: MILP should have LOWER energy\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"VERIFICATION: Energy of MILP vs Generated Candidates\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "ebm.eval()\n",
        "\n",
        "# Collect energies on eval set\n",
        "milp_energies = []\n",
        "candidate_energies = []\n",
        "energy_gaps = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
        "        B = len(batch[\"scenario_ids\"])\n",
        "\n",
        "        # Get embeddings and MILP decisions\n",
        "        h = batch[\"embeddings\"].to(config.device)\n",
        "        u_positive = batch[\"milp_decisions\"].to(config.device)\n",
        "\n",
        "        n_zones = batch[\"n_zones\"]\n",
        "        n_timesteps = batch[\"n_timesteps\"]\n",
        "\n",
        "        # Handle embedding format for EBM\n",
        "        if h.dim() == 3:\n",
        "            h_flat = h.mean(dim=1)\n",
        "        elif h.dim() == 4:\n",
        "            h_flat = h.mean(dim=(1, 2))\n",
        "        else:\n",
        "            h_flat = h\n",
        "\n",
        "        # Compute energy of MILP decisions\n",
        "        u_pos_flat = u_positive.view(B, -1)\n",
        "        e_milp = ebm(u_pos_flat, h_flat)  # [B]\n",
        "        milp_energies.extend(e_milp.cpu().tolist())\n",
        "\n",
        "        # Sample and evaluate K random candidates\n",
        "        K = config.num_candidates\n",
        "        u_candidates, _ = sampler.sample(\n",
        "            h=h,\n",
        "            n_samples=K,\n",
        "            n_zones=n_zones,\n",
        "            n_timesteps=n_timesteps,\n",
        "            n_features=config.n_features,\n",
        "        )\n",
        "        u_binary = binarize_decisions(u_candidates)\n",
        "\n",
        "        # Compute energy of candidates\n",
        "        u_neg_flat = u_binary.view(B * K, -1)\n",
        "        h_exp = h_flat.unsqueeze(1).expand(-1, K, -1).reshape(B * K, -1)\n",
        "        e_candidates = ebm(u_neg_flat, h_exp).view(B, K)  # [B, K]\n",
        "\n",
        "        # Store mean energy per scenario\n",
        "        candidate_energies.extend(e_candidates.mean(dim=1).cpu().tolist())\n",
        "\n",
        "        # Energy gap per scenario (should be positive if EBM learned correctly)\n",
        "        gaps = e_candidates.mean(dim=1) - e_milp\n",
        "        energy_gaps.extend(gaps.cpu().tolist())\n",
        "\n",
        "# Compute statistics\n",
        "milp_energies = np.array(milp_energies)\n",
        "candidate_energies = np.array(candidate_energies)\n",
        "energy_gaps = np.array(energy_gaps)\n",
        "\n",
        "print(f\"\\nüìä Energy Statistics:\")\n",
        "print(f\"   MILP solutions (u+):      mean = {milp_energies.mean():.3f}, std = {milp_energies.std():.3f}\")\n",
        "print(f\"   Generated candidates (u-): mean = {candidate_energies.mean():.3f}, std = {candidate_energies.std():.3f}\")\n",
        "print(f\"   Energy gap (E- - E+):     mean = {energy_gaps.mean():.3f}, std = {energy_gaps.std():.3f}\")\n",
        "\n",
        "# Check if training was successful\n",
        "pct_correct = (energy_gaps > 0).mean() * 100\n",
        "print(f\"\\n‚úì MILP has lower energy in {pct_correct:.1f}% of scenarios\")\n",
        "\n",
        "if energy_gaps.mean() > 0:\n",
        "    print(\"‚úÖ EBM TRAINED CORRECTLY: MILP solutions have lower energy on average\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: EBM may need more training - candidates have lower energy than MILP\")\n",
        "\n",
        "# Plot energy distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# 1. Energy histogram\n",
        "ax1 = axes[0]\n",
        "ax1.hist(milp_energies, bins=30, alpha=0.7, label=\"MILP (u+)\", color=\"green\")\n",
        "ax1.hist(candidate_energies, bins=30, alpha=0.7, label=\"Candidates (u-)\", color=\"red\")\n",
        "ax1.set_xlabel(\"Energy\")\n",
        "ax1.set_ylabel(\"Count\")\n",
        "ax1.set_title(\"Energy Distribution\")\n",
        "ax1.legend()\n",
        "ax1.axvline(milp_energies.mean(), color=\"green\", linestyle=\"--\", linewidth=2)\n",
        "ax1.axvline(candidate_energies.mean(), color=\"red\", linestyle=\"--\", linewidth=2)\n",
        "\n",
        "# 2. Energy gap histogram\n",
        "ax2 = axes[1]\n",
        "ax2.hist(energy_gaps, bins=30, alpha=0.7, color=\"blue\")\n",
        "ax2.axvline(0, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Zero (boundary)\")\n",
        "ax2.axvline(energy_gaps.mean(), color=\"green\", linestyle=\"--\", linewidth=2, label=f\"Mean: {energy_gaps.mean():.2f}\")\n",
        "ax2.set_xlabel(\"Energy Gap (E- - E+)\")\n",
        "ax2.set_ylabel(\"Count\")\n",
        "ax2.set_title(f\"Energy Gap Distribution\\n{pct_correct:.0f}% scenarios with positive gap\")\n",
        "ax2.legend()\n",
        "\n",
        "# 3. Scatter: MILP energy vs Candidate energy\n",
        "ax3 = axes[2]\n",
        "ax3.scatter(milp_energies, candidate_energies, alpha=0.5, s=20)\n",
        "ax3.plot([milp_energies.min(), milp_energies.max()],\n",
        "         [milp_energies.min(), milp_energies.max()],\n",
        "         'r--', linewidth=2, label=\"y=x (equal energy)\")\n",
        "ax3.set_xlabel(\"E(MILP)\")\n",
        "ax3.set_ylabel(\"E(Candidates)\")\n",
        "ax3.set_title(\"MILP vs Candidate Energy\\nPoints above line = EBM correct\")\n",
        "ax3.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(Path(config.output_dir) / \"energy_verification.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úì Verification plot saved to: {config.output_dir}/energy_verification.png\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "186c5363609d48099066f542bacfcaae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_863a80c645c34101a12d17d8866d6f74",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b1ad73501fa0471dbd847d9474a6d209",
            "value": "Epoch‚Äá1/50:‚Äá‚Äá‚Äá1%"
          }
        },
        "507b8388f4804f40a1d56e4c350d10e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5728ae23ecd04b4fbd251e29d0b4923c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eece68bd0bfa4598b1404638ec218245",
            "max": 1184,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90797683e0dd44e1871bfd5ac53d14e5",
            "value": 13
          }
        },
        "863a80c645c34101a12d17d8866d6f74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90797683e0dd44e1871bfd5ac53d14e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f34a6f58b504d0291e2543a9781dc45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_186c5363609d48099066f542bacfcaae",
              "IPY_MODEL_5728ae23ecd04b4fbd251e29d0b4923c",
              "IPY_MODEL_e3e22e5a453b4d669611fd18f8dcd6f4"
            ],
            "layout": "IPY_MODEL_507b8388f4804f40a1d56e4c350d10e5"
          }
        },
        "aa2dc7da3efd455eab9153b88492a218": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1ad73501fa0471dbd847d9474a6d209": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc04af65558f4142891b33eab48c2839": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3e22e5a453b4d669611fd18f8dcd6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc04af65558f4142891b33eab48c2839",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_aa2dc7da3efd455eab9153b88492a218",
            "value": "‚Äá13/1184‚Äá[41:14&lt;50:24:39,‚Äá154.98s/it,‚Äáloss=5.9931,‚ÄáE+=-0.00,‚ÄáE-=0.00]"
          }
        },
        "eece68bd0bfa4598b1404638ec218245": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
