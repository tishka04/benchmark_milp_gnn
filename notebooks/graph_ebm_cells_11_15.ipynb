{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Plot Training Curves (Graph EBM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Loss\n",
        "axes[0, 0].plot(history['train_loss'], label='Train', linewidth=2)\n",
        "if len(history.get('val_loss', [])) > 0:\n",
        "    val_epochs = [i * CONFIG['validate_every'] for i in range(len(history['val_loss']))]\n",
        "    axes[0, 0].plot(val_epochs, history['val_loss'], label='Val', marker='o', linewidth=2)\n",
        "axes[0, 0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Energy gap (E_pos - E_neg)\n",
        "axes[0, 1].plot(history['train_gap'], label='Train', linewidth=2, color='green')\n",
        "if len(history.get('val_gap', [])) > 0:\n",
        "    val_epochs = [i * CONFIG['validate_every'] for i in range(len(history['val_gap']))]\n",
        "    axes[0, 1].plot(val_epochs, history['val_gap'], label='Val', marker='o', linewidth=2, color='darkgreen')\n",
        "axes[0, 1].set_title('Energy Gap (E_pos - E_neg)', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Gap', fontsize=12)\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Positive energy\n",
        "axes[1, 0].plot(history['train_e_pos'], linewidth=2, color='blue')\n",
        "axes[1, 0].set_title('Positive Sample Energy', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1, 0].set_ylabel('E(u_pos | graph)', fontsize=12)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Negative energy\n",
        "axes[1, 1].plot(history['train_e_neg'], linewidth=2, color='red')\n",
        "axes[1, 1].set_title('Negative Sample Energy', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1, 1].set_ylabel('E(u_neg | graph)', fontsize=12)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(CONFIG['output_dir'], 'graph_ebm_training_curves.png'), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✓ Training curves saved to {os.path.join(CONFIG['output_dir'], 'graph_ebm_training_curves.png')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Save Training History (Graph EBM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Save history as JSON\n",
        "history_path = os.path.join(CONFIG['output_dir'], 'graph_ebm_training_history.json')\n",
        "\n",
        "with open(history_path, 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "print(f\"✓ Training history saved to {history_path}\")\n",
        "\n",
        "# Save config\n",
        "config_path = os.path.join(CONFIG['output_dir'], 'graph_ebm_config.json')\n",
        "with open(config_path, 'w') as f:\n",
        "    # Convert non-serializable types\n",
        "    config_save = {k: str(v) if not isinstance(v, (int, float, str, bool, list, dict, type(None))) else v \n",
        "                   for k, v in CONFIG.items()}\n",
        "    json.dump(config_save, f, indent=2)\n",
        "\n",
        "print(f\"✓ Config saved to {config_path}\")\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total epochs: {len(history['train_loss'])}\")\n",
        "print(f\"Best validation gap: {max(history.get('val_gap', [0])):.4f}\")\n",
        "print(f\"Final training loss: {history['train_loss'][-1]:.4f}\")\n",
        "print(f\"Final training gap: {history['train_gap'][-1]:.4f}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Evaluate Best Model (Graph EBM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluating best Graph EBM model...\\n\")\n",
        "\n",
        "# Load best checkpoint\n",
        "best_path = os.path.join(CONFIG['output_dir'], 'graph_ebm_best.pt')\n",
        "if os.path.exists(best_path):\n",
        "    graph_model.load_state_dict(torch.load(best_path))\n",
        "    print(f\"✓ Loaded best model from {best_path}\")\n",
        "else:\n",
        "    print(f\"⚠️  Best model not found, using current model\")\n",
        "\n",
        "# Detailed validation\n",
        "graph_model.eval()\n",
        "\n",
        "all_energy_pos = []\n",
        "all_energy_neg = []\n",
        "all_gaps = []\n",
        "all_graph_sizes = []\n",
        "\n",
        "print(\"Running evaluation on validation set...\")\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "        batch = batch.to(CONFIG['device'])\n",
        "        \n",
        "        # Positive energy (ground truth)\n",
        "        E_pos = graph_model(batch)\n",
        "        \n",
        "        # Sample negatives\n",
        "        u_neg = graph_sampler.sample(batch)\n",
        "        batch_neg = batch.clone()\n",
        "        batch_neg.x = u_neg\n",
        "        E_neg = graph_model(batch_neg)\n",
        "        \n",
        "        # Compute metrics\n",
        "        energy_gap = (E_pos.mean() - E_neg.mean()).item()\n",
        "        \n",
        "        # Store results\n",
        "        all_energy_pos.append(E_pos.mean().item())\n",
        "        all_energy_neg.append(E_neg.mean().item())\n",
        "        all_gaps.append(energy_gap)\n",
        "        \n",
        "        # Track graph sizes for analysis\n",
        "        batch_size = batch.h.shape[0]\n",
        "        avg_nodes = batch.x.shape[0] / batch_size\n",
        "        all_graph_sizes.append(avg_nodes)\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATION RESULTS (GRAPH EBM)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Energy (positive):    {np.mean(all_energy_pos):.4f} ± {np.std(all_energy_pos):.4f}\")\n",
        "print(f\"Energy (negative):    {np.mean(all_energy_neg):.4f} ± {np.std(all_energy_neg):.4f}\")\n",
        "print(f\"Energy gap:           {np.mean(all_gaps):.4f} ± {np.std(all_gaps):.4f}\")\n",
        "print(f\"\\nGraph Statistics:\")\n",
        "print(f\"  Avg nodes per graph: {np.mean(all_graph_sizes):.1f}\")\n",
        "print(f\"  Total batches:       {len(all_gaps)}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save evaluation results\n",
        "eval_results = {\n",
        "    'energy_pos_mean': float(np.mean(all_energy_pos)),\n",
        "    'energy_pos_std': float(np.std(all_energy_pos)),\n",
        "    'energy_neg_mean': float(np.mean(all_energy_neg)),\n",
        "    'energy_neg_std': float(np.std(all_energy_neg)),\n",
        "    'energy_gap_mean': float(np.mean(all_gaps)),\n",
        "    'energy_gap_std': float(np.std(all_gaps)),\n",
        "    'avg_graph_size': float(np.mean(all_graph_sizes)),\n",
        "    'num_batches': len(all_gaps),\n",
        "}\n",
        "\n",
        "eval_path = os.path.join(CONFIG['output_dir'], 'graph_ebm_evaluation_results.json')\n",
        "with open(eval_path, 'w') as f:\n",
        "    json.dump(eval_results, f, indent=2)\n",
        "\n",
        "print(f\"\\n✓ Evaluation results saved to {eval_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Sample and Analyze Configurations (Graph EBM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Sampling configurations from Graph EBM...\\n\")\n",
        "\n",
        "# Get a test graph from validation set\n",
        "test_batch = next(iter(val_loader)).to(CONFIG['device'])\n",
        "print(f\"Test batch size: {test_batch.h.shape[0]} graphs\")\n",
        "print(f\"Total nodes: {test_batch.x.shape[0]}\")\n",
        "print(f\"Nodes per graph (avg): {test_batch.x.shape[0] / test_batch.h.shape[0]:.1f}\")\n",
        "\n",
        "# Sample multiple configurations for the same graph\n",
        "num_samples = 10\n",
        "print(f\"\\nGenerating {num_samples} samples...\")\n",
        "\n",
        "sampled_energies = []\n",
        "sampled_configs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Get ground truth energy\n",
        "    E_true = graph_model(test_batch)\n",
        "    print(f\"Ground truth energy: {E_true.mean().item():.4f}\")\n",
        "    \n",
        "    # Sample multiple configurations\n",
        "    for i in range(num_samples):\n",
        "        u_sample = graph_sampler.sample(test_batch)\n",
        "        batch_sample = test_batch.clone()\n",
        "        batch_sample.x = u_sample\n",
        "        E_sample = graph_model(batch_sample)\n",
        "        \n",
        "        sampled_energies.append(E_sample.mean().item())\n",
        "        sampled_configs.append(u_sample.cpu().numpy())\n",
        "\n",
        "# Analyze samples\n",
        "print(f\"\\n{num_samples} samples generated:\")\n",
        "print(f\"Energy range: [{min(sampled_energies):.4f}, {max(sampled_energies):.4f}]\")\n",
        "print(f\"Energy mean: {np.mean(sampled_energies):.4f}\")\n",
        "print(f\"Energy std: {np.std(sampled_energies):.4f}\")\n",
        "\n",
        "# Check diversity (average Hamming distance between samples)\n",
        "if num_samples > 1:\n",
        "    hamming_distances = []\n",
        "    for i in range(num_samples):\n",
        "        for j in range(i+1, num_samples):\n",
        "            dist = np.mean(sampled_configs[i] != sampled_configs[j])\n",
        "            hamming_distances.append(dist)\n",
        "    \n",
        "    avg_diversity = np.mean(hamming_distances)\n",
        "    print(f\"\\nSample diversity (avg Hamming): {avg_diversity:.4f}\")\n",
        "\n",
        "# Visualize first sample\n",
        "sample_u = sampled_configs[0].flatten()\n",
        "plt.figure(figsize=(15, 4))\n",
        "\n",
        "# Plot first 500 variables or all if less\n",
        "n_plot = min(500, len(sample_u))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(sample_u[:n_plot], 'o-', markersize=2, linewidth=0.5)\n",
        "plt.title(f'Sample Binary Configuration (first {n_plot} variables)', fontsize=12, fontweight='bold')\n",
        "plt.xlabel('Variable Index')\n",
        "plt.ylabel('Value')\n",
        "plt.ylim([-0.1, 1.1])\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot histogram of values\n",
        "plt.subplot(1, 2, 2)\n",
        "unique, counts = np.unique(sample_u, return_counts=True)\n",
        "plt.bar(unique, counts)\n",
        "plt.title('Distribution of Binary Values', fontsize=12, fontweight='bold')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1])\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add sparsity info\n",
        "sparsity = np.mean(sample_u == 0)\n",
        "plt.text(0.5, max(counts)*0.9, f'Sparsity: {sparsity:.1%}', \n",
        "         ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(CONFIG['output_dir'], 'graph_ebm_sample_configuration.png'), dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✓ Sample visualization saved\")\n",
        "print(f\"\\nSample statistics:\")\n",
        "print(f\"  Total variables: {len(sample_u)}\")\n",
        "print(f\"  Sparsity (% zeros): {sparsity:.1%}\")\n",
        "print(f\"  Active variables: {np.sum(sample_u == 1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Summary and Next Steps (Graph EBM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count model parameters\n",
        "num_params = sum(p.numel() for p in graph_model.parameters())\n",
        "num_trainable = sum(p.numel() for p in graph_model.parameters() if p.requires_grad)\n",
        "\n",
        "# Get dataset info\n",
        "try:\n",
        "    dataset_size = len(train_loader.dataset) + len(val_loader.dataset)\n",
        "except:\n",
        "    dataset_size = \"Unknown\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"GRAPH EBM TRAINING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"  Model Type: Graph Energy Model (Deep Sets)\")\n",
        "print(f\"  Parameters: {num_params:,}\")\n",
        "print(f\"  Trainable: {num_trainable:,}\")\n",
        "print(f\"  Hidden Dims: {CONFIG['hidden_dims']}\")\n",
        "print(f\"  Activation: {CONFIG['activation']}\")\n",
        "print(f\"  Dropout: {CONFIG['dropout']}\")\n",
        "\n",
        "print(f\"\\nTraining Configuration:\")\n",
        "print(f\"  Epochs: {len(history['train_loss'])}\")\n",
        "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"  Dataset size: {dataset_size}\")\n",
        "print(f\"  Device: {CONFIG['device']}\")\n",
        "\n",
        "print(f\"\\nTraining Results:\")\n",
        "print(f\"  Best validation gap: {max(history.get('val_gap', [0])):.4f}\")\n",
        "print(f\"  Final training loss: {history['train_loss'][-1]:.4f}\")\n",
        "print(f\"  Final training gap: {history['train_gap'][-1]:.4f}\")\n",
        "print(f\"  Final E_pos: {history['train_e_pos'][-1]:.4f}\")\n",
        "print(f\"  Final E_neg: {history['train_e_neg'][-1]:.4f}\")\n",
        "\n",
        "if 'energy_gap_mean' in eval_results:\n",
        "    print(f\"\\nEvaluation Metrics:\")\n",
        "    print(f\"  Energy gap (mean): {eval_results['energy_gap_mean']:.4f} ± {eval_results['energy_gap_std']:.4f}\")\n",
        "    print(f\"  Avg graph size: {eval_results['avg_graph_size']:.1f} nodes\")\n",
        "\n",
        "print(f\"\\nSaved Artifacts:\")\n",
        "print(f\"  Best model: {os.path.join(CONFIG['output_dir'], 'graph_ebm_best.pt')}\")\n",
        "print(f\"  Final model: {os.path.join(CONFIG['output_dir'], 'graph_ebm_final.pt')}\")\n",
        "print(f\"  Training history: {os.path.join(CONFIG['output_dir'], 'graph_ebm_training_history.json')}\")\n",
        "print(f\"  Evaluation results: {os.path.join(CONFIG['output_dir'], 'graph_ebm_evaluation_results.json')}\")\n",
        "print(f\"  Training curves: {os.path.join(CONFIG['output_dir'], 'graph_ebm_training_curves.png')}\")\n",
        "print(f\"  Sample visualization: {os.path.join(CONFIG['output_dir'], 'graph_ebm_sample_configuration.png')}\")\n",
        "\n",
        "print(f\"\\nNext Steps:\")\n",
        "print(f\"  1. Analyze training curves and energy gaps\")\n",
        "print(f\"  2. Test on different graph sizes and scenarios\")\n",
        "print(f\"  3. Compare with flat EBM baseline\")\n",
        "print(f\"  4. Tune sampling strategy (exact vs approximate)\")\n",
        "print(f\"  5. Use sampled configurations for MILP warmstart\")\n",
        "print(f\"  6. Evaluate on unseen scenarios\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ GRAPH EBM TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Optional: Show model summary\n",
        "print(f\"\\nModel Summary:\")\n",
        "print(graph_model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
